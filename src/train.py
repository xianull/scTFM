import pyrootutils
import torch.multiprocessing as mp
import torch
import os
import json
import tempfile
from pathlib import Path

root = pyrootutils.setup_root(
    search_from=__file__,
    indicator=[".git", "requirements.txt"],
    pythonpath=True,
    dotenv=True,
)

import hydra
import wandb
from omegaconf import DictConfig, OmegaConf
import pytorch_lightning as pl
from pytorch_lightning import Callback, LightningDataModule, LightningModule, Trainer
from pytorch_lightning.loggers import Logger
from pytorch_lightning.loggers.wandb import WandbLogger
from typing import List, Optional

from src.utils.pylogger import get_pylogger
from src.utils.dataset_stats_utils import get_dataset_stats

log = get_pylogger(__name__)


def infer_latent_dim_from_ae(cfg: DictConfig) -> None:
    """
    ä» AE checkpoint æ¨æ–­ latent_dim å¹¶è®¾ç½® input_dimã€‚

    æ­¤å‡½æ•°åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ‰§è¡Œï¼ˆåŒ…æ‹¬ DDP å­è¿›ç¨‹ï¼‰ï¼Œ
    ä»¥ç¡®ä¿æ¨¡å‹å‚æ•°å½¢çŠ¶ä¸€è‡´ã€‚
    """
    if not cfg.get("train"):
        return

    mode = cfg.model.get("mode")
    if mode != "latent":
        return

    ae_ckpt_path = cfg.model.get("ae_ckpt_path")
    if not ae_ckpt_path:
        return

    try:
        from pathlib import Path
        import yaml

        ckpt_dir = Path(ae_ckpt_path).parent.parent
        hydra_config_path = ckpt_dir / ".hydra" / "config.yaml"

        if hydra_config_path.exists():
            with open(hydra_config_path, 'r') as f:
                ae_cfg = yaml.safe_load(f)

            latent_dim = ae_cfg.get('model', {}).get('net', {}).get('latent_dim')
            if latent_dim:
                OmegaConf.set_struct(cfg, False)
                cfg.model.net.input_dim = latent_dim
                OmegaConf.set_struct(cfg, True)
                log.info(f"ğŸ”§ è‡ªåŠ¨è®¾ç½® input_dim={latent_dim} (ä» AE checkpoint è¯»å–)")
    except Exception as e:
        log.warning(f"âš ï¸ æ— æ³•ä» AE checkpoint æ¨æ–­ latent_dim: {e}")


def ensure_latent_data_if_needed(cfg: DictConfig, do_extract: bool = True) -> None:
    """
    æ ¹æ® model.mode è‡ªåŠ¨å¤„ç† latent æ•°æ®ï¼ˆä»…é€‚ç”¨äº RTF è®­ç»ƒï¼‰ã€‚

    æ­¤å‡½æ•°åªåœ¨ä»¥ä¸‹æ¡ä»¶åŒæ—¶æ»¡è¶³æ—¶æ‰§è¡Œï¼š
    1. cfg.train = Trueï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
    2. cfg.model.mode å­˜åœ¨ï¼ˆRTF æ¨¡å‹ç‰¹æœ‰é…ç½®ï¼ŒAE æ¨¡å‹æ²¡æœ‰æ­¤å­—æ®µï¼‰

    é€»è¾‘ï¼š
    - mode=raw: ç›´æ¥ä½¿ç”¨ raw_data_dir
    - mode=latent: æ£€æŸ¥ latent æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨æå–

    Args:
        cfg: Hydra é…ç½®å¯¹è±¡
        do_extract: æ˜¯å¦æ‰§è¡Œå®é™…æå–ï¼ˆDDP å­è¿›ç¨‹è®¾ä¸º Falseï¼Œåªåšè·¯å¾„æ¨ç†ï¼‰
    """
    # åªåœ¨è®­ç»ƒæ—¶å¤„ç†
    if not cfg.get("train"):
        return

    # æ£€æŸ¥æ˜¯å¦æ˜¯ RTF è®­ç»ƒï¼ˆAE æ¨¡å‹æ²¡æœ‰ mode é…ç½®ï¼Œä¼šç›´æ¥è¿”å›ï¼‰
    mode = cfg.model.get("mode")
    if mode is None:
        # AE è®­ç»ƒæˆ–å…¶ä»–æ²¡æœ‰ mode çš„æ¨¡å‹ï¼Œè·³è¿‡
        return

    # è·å– raw_data_dir
    raw_data_dir = cfg.data.get("raw_data_dir")
    if raw_data_dir is None:
        # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ raw_data_dirï¼Œä½¿ç”¨ data_dir
        raw_data_dir = cfg.data.get("data_dir")
        if raw_data_dir is None:
            return

    from src.utils.latent_manager import ensure_latent_data, get_latent_dir

    # è·å–é…ç½®
    ae_ckpt_path = cfg.model.get("ae_ckpt_path")
    latent_dir = cfg.data.get("latent_dir")

    try:
        if do_extract:
            # ä¸»è¿›ç¨‹ï¼šå®Œæ•´æµç¨‹ï¼ˆæ£€æŸ¥ + å¿…è¦æ—¶æå–ï¼‰
            actual_data_dir = ensure_latent_data(
                mode=mode,
                raw_data_dir=raw_data_dir,
                ae_ckpt_path=ae_ckpt_path,
                latent_dir=latent_dir,
                batch_size=2048,  # æå–æ—¶ä½¿ç”¨è¾ƒå¤§ batch
                device="cuda" if torch.cuda.is_available() else "cpu",
            )
        else:
            # DDP å­è¿›ç¨‹ï¼šä»…æ¨ç†è·¯å¾„ï¼Œä¸æ‰§è¡Œæå–
            if mode == "raw":
                actual_data_dir = raw_data_dir
            elif mode == "latent":
                actual_data_dir = latent_dir if latent_dir else get_latent_dir(raw_data_dir)
            else:
                actual_data_dir = raw_data_dir

        # æ›´æ–°é…ç½®ä¸­çš„ data_dir
        OmegaConf.set_struct(cfg, False)
        cfg.data.data_dir = actual_data_dir
        OmegaConf.set_struct(cfg, True)

        log.info(f"ğŸ“ æ•°æ®ç›®å½•å·²è®¾ç½®: {actual_data_dir}")

    except Exception as e:
        log.error(f"âŒ Latent æ•°æ®å¤„ç†å¤±è´¥: {e}")
        raise


def log_hyperparameters_to_wandb(
    cfg: DictConfig,
    loggers: List[Logger],
) -> None:
    """å°†Hydraé…ç½®è®°å½•åˆ°WandBã€‚"""
    wandb_logger: Optional[WandbLogger] = None
    for logger in loggers:
        if isinstance(logger, WandbLogger):
            wandb_logger = logger
            break

    if wandb_logger is None:
        log.warning("No WandbLogger found, skipping hyperparameter logging.")
        return

    experiment = wandb_logger.experiment
    if not hasattr(experiment, 'config') or not hasattr(experiment.config, 'update'):
        return

    hparams = {}
    config_keys = ["model", "data", "trainer", "callbacks", "task_name", "seed", "train", "test"]
    for key in config_keys:
        if key in cfg:
            value = cfg[key]
            if OmegaConf.is_config(value):
                hparams[key] = OmegaConf.to_container(value, resolve=True)
            else:
                hparams[key] = value

    experiment.config.update(hparams, allow_val_change=True)
    log.info("Hyperparameters logged to WandB successfully.")

@hydra.main(version_base="1.3", config_path="../configs", config_name="train.yaml")
def main(cfg: DictConfig):
    # ---------------------------------------------------------------------------
    # [å…³é”®] è‡ªåŠ¨æ¨æ–­ latent_dimï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½æ‰§è¡Œï¼Œç¡®ä¿æ¨¡å‹å‚æ•°ä¸€è‡´ï¼‰
    # ---------------------------------------------------------------------------
    infer_latent_dim_from_ae(cfg)

    # ---------------------------------------------------------------------------
    # [æ–°å¢] è‡ªåŠ¨å¤„ç† Latent æ•°æ®
    # - ä¸»è¿›ç¨‹ï¼ˆDDP spawn å‰ï¼‰ï¼šæ£€æŸ¥å¹¶æå– latent æ•°æ®
    # - DDP å­è¿›ç¨‹ï¼šä»…æ›´æ–° data_dirï¼Œä¸æ‰§è¡Œå®é™…æå–
    # ---------------------------------------------------------------------------
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    rank = int(os.environ.get("RANK", -1))
    is_pre_ddp = (local_rank == -1 and rank == -1)

    if cfg.get("train"):
        # æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ›´æ–° data_dirï¼ˆDDP å­è¿›ç¨‹åªåšè·¯å¾„æ¨ç†ï¼Œä¸æ‰§è¡Œæå–ï¼‰
        ensure_latent_data_if_needed(cfg, do_extract=is_pre_ddp)

    # ---------------------------------------------------------------------------
    # [å…³é”®] ä½¿ç”¨æ–‡ä»¶ç¼“å­˜é¿å… DDP å¤šè¿›ç¨‹é‡å¤è®¡ç®—
    # ä¸»è¿›ç¨‹ï¼ˆspawn å‰ï¼‰è®¡ç®—å¹¶ä¿å­˜ï¼Œå­è¿›ç¨‹ï¼ˆspawn åï¼‰ç›´æ¥è¯»å–
    # ---------------------------------------------------------------------------

    if cfg.get("train"):
        try:
            data_dir = cfg.data.get("data_dir")
            batch_size = cfg.data.get("batch_size", 256)

            # è®¡ç®— World Size
            devices = cfg.trainer.get("devices")
            if devices == "auto":
                world_size = torch.cuda.device_count()
            elif isinstance(devices, (list, tuple)) or OmegaConf.is_list(devices):
                world_size = len(devices)
            elif isinstance(devices, int):
                world_size = devices
            elif isinstance(devices, str) and devices.isdigit():
                world_size = int(devices)
            else:
                if isinstance(devices, str) and "," in devices:
                    world_size = len(devices.split(","))
                else:
                    world_size = 1

            # ä½¿ç”¨ task_name + æ•°æ®ç›®å½• hash ä½œä¸ºç¼“å­˜ç›®å½•ï¼Œé¿å…ä¸åŒä»»åŠ¡/æ•°æ®é›†å†²çª
            task_name = cfg.get("task_name", "default")
            import hashlib
            data_hash = hashlib.md5(data_dir.encode()).hexdigest()[:8]
            cache_dir = Path(tempfile.gettempdir()) / "scTFM_cache" / task_name / data_hash
            cache_dir.mkdir(parents=True, exist_ok=True)
            cache_file = cache_dir / f"stats_bs{batch_size}_ws{world_size}.json"

            # è·å– DataLoader workers æ•°ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
            num_workers_per_gpu = cfg.data.get("num_workers", 16)
            
            if is_pre_ddp:
                # ä¸»è¿›ç¨‹ï¼šè®¡ç®—å¹¶ç¼“å­˜
                log.info(f"ğŸ“Š [Main Process] Calculating dataset stats (World Size={world_size})...")
                
                from src.utils.dataset_stats_utils import balanced_shard_assignment
                
                # ä¸€è‡´æ€§è®­ç»ƒä»»åŠ¡ä¸ä½¿ç”¨ split_label ç­›é€‰
                split_label = None if task_name == "consistency_flow" else 0
                
                total_cells, total_steps, shard_sizes = get_dataset_stats(
                    root_dir=data_dir,
                    split_label=split_label, 
                    batch_size=batch_size,
                    num_workers=16,  # åªæœ‰ä¸€ä¸ªè¿›ç¨‹è®¡ç®—ï¼Œå¯ä»¥å¼€å¤§
                    world_size=world_size,
                    num_workers_per_gpu=num_workers_per_gpu
                )
                
                # è®¡ç®—è´Ÿè½½å‡è¡¡çš„åˆ†é…æ–¹æ¡ˆ
                total_workers = world_size * num_workers_per_gpu
                assignment = balanced_shard_assignment(shard_sizes, total_workers)
                
                # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                cache_file.write_text(json.dumps({
                    "total_cells": total_cells,
                    "total_steps": total_steps,
                    "world_size": world_size,
                    "batch_size": batch_size,
                    "num_workers_per_gpu": num_workers_per_gpu,
                    "shard_sizes": shard_sizes,
                    "assignment": {str(k): v for k, v in assignment.items()}  # JSON keys must be strings
                }))
                
                log.info(f"âœ… [Main] Cached stats + assignment: {total_steps} steps â†’ {cache_file}")
                
            else:
                # DDP å­è¿›ç¨‹ï¼šè¯»å–ç¼“å­˜
                if cache_file.exists():
                    stats = json.loads(cache_file.read_text())
                    total_steps = stats["total_steps"]
                    log.info(f"ğŸ“¥ [Rank {local_rank}] Loaded from cache: {total_steps} steps")
                else:
                    log.warning(f"âš ï¸ [Rank {local_rank}] Cache not found, recalculating...")
                    from src.utils.dataset_stats_utils import balanced_shard_assignment
                    split_label = None if task_name == "consistency_flow" else 0
                    total_cells, total_steps, shard_sizes = get_dataset_stats(
                        root_dir=data_dir, split_label=split_label, 
                        batch_size=batch_size, num_workers=4, world_size=world_size,
                        num_workers_per_gpu=num_workers_per_gpu
                    )
            
            # è®¾ç½®é…ç½®
            # ä¸€è‡´æ€§è®­ç»ƒçš„ batch æ•°å–å†³äºé“¾æ•°é‡è€Œéç»†èƒæ•°ï¼Œä¸ä½¿ç”¨ limit_train_batches
            if total_steps > 0 and task_name != "consistency_flow":
                OmegaConf.set_struct(cfg, False)
                cfg.trainer.limit_train_batches = total_steps
                OmegaConf.set_struct(cfg, True)
                
        except Exception as e:
            log.warning(f"âŒ Failed to handle dataset stats: {e}")

    # ---------------------------------------------------------------------------
    # [Hydra/Lightning åˆå§‹åŒ–]
    # ---------------------------------------------------------------------------

    # 1. Seed
    if cfg.get("seed"):
        pl.seed_everything(cfg.seed, workers=True)

    # 2. DataModule
    log.info(f"Instantiating datamodule <{cfg.data._target_}>")

    # å¦‚æœæœ‰ç¼“å­˜çš„è´Ÿè½½å‡è¡¡æ–¹æ¡ˆï¼Œæ³¨å…¥åˆ° DataModule
    if cfg.get("train"):
        try:
            data_dir = cfg.data.get("data_dir")
            batch_size = cfg.data.get("batch_size", 256)
            devices = cfg.trainer.get("devices")

            if devices == "auto":
                world_size = torch.cuda.device_count()
            elif isinstance(devices, (list, tuple)) or OmegaConf.is_list(devices):
                world_size = len(devices)
            elif isinstance(devices, int):
                world_size = devices
            elif isinstance(devices, str) and devices.isdigit():
                world_size = int(devices)
            else:
                if isinstance(devices, str) and "," in devices:
                    world_size = len(devices.split(","))
                else:
                    world_size = 1

            # ä½¿ç”¨ä¸ä¸Šé¢ä¸€è‡´çš„ cache è·¯å¾„
            task_name = cfg.get("task_name", "default")
            import hashlib
            data_hash = hashlib.md5(data_dir.encode()).hexdigest()[:8]
            cache_dir = Path(tempfile.gettempdir()) / "scTFM_cache" / task_name / data_hash
            cache_file = cache_dir / f"stats_bs{batch_size}_ws{world_size}.json"

            if cache_file.exists():
                stats = json.loads(cache_file.read_text())
                if "assignment" in stats:
                    # æ³¨å…¥è´Ÿè½½å‡è¡¡æ–¹æ¡ˆåˆ°é…ç½®
                    # [CRITICAL] ä¿æŒå­—ç¬¦ä¸² keyï¼Œå› ä¸º Dataset æŸ¥æ‰¾æ—¶ç”¨ str(global_worker_id)
                    OmegaConf.set_struct(cfg, False)
                    cfg.data.shard_assignment = stats["assignment"]  # ä¿æŒåŸå§‹å­—ç¬¦ä¸² key
                    OmegaConf.set_struct(cfg, True)
                    log.info(f"ğŸ“¥ Loaded shard assignment from cache ({len(stats['assignment'])} workers)")
        except Exception as e:
            log.warning(f"Failed to load shard assignment: {e}")
    
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)

    # 4. Model
    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    # 5. Callbacks
    callbacks: List[Callback] = []
    if cfg.get("callbacks"):
        for _, cb_conf in cfg.callbacks.items():
            if "_target_" in cb_conf:
                callbacks.append(hydra.utils.instantiate(cb_conf))

    # 6. Logger
    logger: List[Logger] = []
    if cfg.get("logger"):
        for _, lg_conf in cfg.logger.items():
            if "_target_" in lg_conf:
                logger.append(hydra.utils.instantiate(lg_conf))

    if logger:
        log_hyperparameters_to_wandb(cfg, logger)

    # 7. Trainer
    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
    trainer: Trainer = hydra.utils.instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=logger,
    )

    # 8. Train
    if cfg.get("train"):
        log.info("Starting training!")
        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))

    # 9. Test
    if cfg.get("test"):
        log.info("Starting testing!")
        ckpt_path = trainer.checkpoint_callback.best_model_path
        if ckpt_path == "":
            log.warning("Best ckpt not found! Using current weights for testing...")
            ckpt_path = None
        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)

    wandb.finish()

if __name__ == "__main__":
    # [å…³é”®] å¿…é¡»è®¾ç½®ä¸º spawnï¼Œå¦åˆ™ ProcessPoolExecutor å’Œ Lightning DDP éƒ½ä¼šå‡ºé—®é¢˜
    mp.set_start_method('spawn', force=True)
    main()
