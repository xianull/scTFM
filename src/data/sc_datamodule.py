from typing import Optional, Dict
import os

import torch
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader

from src.data.components.ae_dataset import SomaCollectionDataset

class SingleCellDataModule(LightningDataModule):
    """
    å•ç»†èƒæ•°æ®çš„ PyTorch Lightning DataModuleï¼Œä½¿ç”¨ SomaCollectionDatasetã€‚
    
    å…³é”®ç‰¹æ€§ï¼š
    Dataset ä¼šç›´æ¥ yield ä¸€ä¸ª batch çš„æ•°æ®ï¼Œå› æ­¤ DataLoader åˆå§‹åŒ–æ—¶å¿…é¡»è®¾ç½® batch_size=Noneã€‚
    
    Split Labels:
    0: Train (ID) - ç”¨äºè®­ç»ƒ
    1: Val (ID)   - ç”¨äºéªŒè¯
    2: Test (ID)  - ç”¨äºæµ‹è¯• (åŒåˆ†å¸ƒ)
    3: Test (OOD) - ç”¨äºæµ‹è¯• (å¤–åˆ†å¸ƒ) - æš‚ä¸éœ€è¦
    """

    def __init__(
        self,
        data_dir: str = "data/",
        batch_size: int = 256,
        num_workers: int = 4,
        pin_memory: bool = True,
        io_chunk_size: int = 16384,
        prefetch_factor: int = 2,
        persistent_workers: bool = True,
        shard_assignment: Optional[Dict] = None,  # æ–°å¢ï¼šè´Ÿè½½å‡è¡¡åˆ†é…æ–¹æ¡ˆ
    ):
        """
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ¯ä¸ª batch çš„å¤§å° (ç›´æ¥ä¼ é€’ç»™ SomaCollectionDataset)
            num_workers: DataLoader çš„ worker æ•°é‡
            pin_memory: æ˜¯å¦å°†æ•°æ®é”åœ¨å†…å­˜ä¸­ (å»ºè®® True)
            io_chunk_size: TileDB è¯»å–æ—¶çš„ chunk å¤§å° (å½±å“å†…å­˜å ç”¨)
            prefetch_factor: æ¯ä¸ª worker é¢„åŠ è½½çš„ batch æ•°é‡
            persistent_workers: æ˜¯å¦ä¿æŒ workers å­˜æ´» (é¿å…é‡å¤åˆå§‹åŒ–å¼€é”€)
            shard_assignment: æ™ºèƒ½è´Ÿè½½å‡è¡¡çš„ shard åˆ†é…æ–¹æ¡ˆ (å¯é€‰)
        """
        super().__init__()

        # å…è®¸é€šè¿‡ self.hparams è®¿é—® init å‚æ•°
        self.save_hyperparameters(logger=False)

        self.data_train: Optional[SomaCollectionDataset] = None
        self.data_val: Optional[SomaCollectionDataset] = None
        # self.data_test: Optional[SomaCollectionDataset] = None # æš‚æ—¶ä¸éœ€è¦Testï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚å¼€å¯
        
        # é¢„æ‰«æ Shards åˆ—è¡¨ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œä¸€æ¬¡ï¼Œé¿å… 64 ä¸ª workers é‡å¤æ‰«æï¼‰
        self._cached_sub_uris: Optional[list] = None

    def setup(self, stage: Optional[str] = None):
        """
        åŠ è½½æ•°æ®ã€‚è®¾ç½®å˜é‡: `self.data_train`, `self.data_val`.
        
        è¿™ä¸ªæ–¹æ³•ä¼šè¢« trainer.fit() å’Œ trainer.test() è°ƒç”¨ã€‚
        æ ¹æ®ç”¨æˆ·æŒ‡ç¤ºï¼Œåªéœ€è¦è¯»å– split_label 0 (Train) å’Œ 1 (Val)ã€‚
        """
        # ä»…å½“æœªåŠ è½½æ—¶æ‰åŠ è½½æ•°æ®é›†
        if not self.data_train and not self.data_val:
            # [å…³é”®ä¼˜åŒ–] åœ¨ä¸»è¿›ç¨‹ä¸­é¢„æ‰«ææ‰€æœ‰ Shardsï¼Œé¿å… 64 ä¸ª workers é‡å¤æ‰«æ
            if self._cached_sub_uris is None:
                print(f"ğŸ” [DataModule] Pre-scanning shards in {self.hparams.data_dir}...")
                self._cached_sub_uris = sorted([
                    os.path.join(self.hparams.data_dir, d) 
                    for d in os.listdir(self.hparams.data_dir) 
                    if os.path.isdir(os.path.join(self.hparams.data_dir, d))
                ])
                print(f"âœ… [DataModule] Found {len(self._cached_sub_uris)} shards (will be shared across all workers)")
            
            # è®­ç»ƒé›† (split_label=0: Train ID)
            self.data_train = SomaCollectionDataset(
                root_dir=self.hparams.data_dir,
                split_label=0,
                io_chunk_size=self.hparams.io_chunk_size,
                batch_size=self.hparams.batch_size,
                preloaded_sub_uris=self._cached_sub_uris,  # ä¼ å…¥é¢„æ‰«æçš„åˆ—è¡¨
                shard_assignment=self.hparams.shard_assignment,  # ä¼ å…¥è´Ÿè½½å‡è¡¡æ–¹æ¡ˆ
            )
            
            # éªŒè¯é›† (split_label=1: Val ID)
            self.data_val = SomaCollectionDataset(
                root_dir=self.hparams.data_dir,
                split_label=1,
                io_chunk_size=self.hparams.io_chunk_size,
                batch_size=self.hparams.batch_size,
                preloaded_sub_uris=self._cached_sub_uris,  # å¤ç”¨åŒä¸€ä¸ªåˆ—è¡¨
                shard_assignment=None,  # éªŒè¯é›†ä¸éœ€è¦è´Ÿè½½å‡è¡¡ï¼ˆæ•°æ®é‡å°ï¼‰
            )
            
            # æ³¨æ„ï¼šsplit_label 2 (Test ID) and 3 (Test OOD) ç›®å‰æœªåŠ è½½
            # å¦‚æœåç»­éœ€è¦æµ‹è¯•ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 

    def train_dataloader(self):
        """è¿”å›è®­ç»ƒé›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_train,
            batch_size=None,  # <--- å…³é”®ï¼Dataset å·²ç»å¤„ç†äº† batching
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers, # ä½¿ç”¨é…ç½®å‚æ•°
        )

    def val_dataloader(self):
        """è¿”å›éªŒè¯é›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_val,
            batch_size=None,  # <--- å…³é”®ï¼
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers,
        )

    def teardown(self, stage: Optional[str] = None):
        """fit æˆ– test ç»“æŸåçš„æ¸…ç†å·¥ä½œ"""
        pass

    def state_dict(self):
        """ä¿å­˜åˆ° checkpoint çš„é¢å¤–çŠ¶æ€"""
        return {}

    def load_state_dict(self, state_dict):
        """åŠ è½½ checkpoint æ—¶çš„æ“ä½œ"""
        pass
