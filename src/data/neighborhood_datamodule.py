"""NeighborhoodDataModule - ç”¨äº SetSCAE çš„å¾®ç¯å¢ƒæ•°æ®æ¨¡å—"""

from typing import Optional, Dict
import os

from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader

from src.data.components.neighborhood_dataset import SomaNeighborhoodDataset


class NeighborhoodDataModule(LightningDataModule):
    """
    å¾®ç¯å¢ƒæ•°æ®çš„ PyTorch Lightning DataModuleã€‚

    ç”¨äºè®­ç»ƒ SetSCAEï¼ˆSet Single-Cell Autoencoderï¼‰ã€‚
    æ¯ä¸ª batch è¿”å› (batch_size, bag_size, n_genes) çš„ç»†èƒé›†åˆã€‚

    Split Labels:
    0: Train (ID) - ç”¨äºè®­ç»ƒ
    1: Val (ID)   - ç”¨äºéªŒè¯
    """

    def __init__(
        self,
        data_dir: str = "data/",
        batch_size: int = 64,
        num_workers: int = 4,
        pin_memory: bool = True,
        io_chunk_size: int = 16384,
        prefetch_factor: int = 2,
        persistent_workers: bool = True,
        split_label_train: int = 0,
        split_label_val: int = 1,
        shard_assignment: Optional[Dict] = None,
        # å¾®ç¯å¢ƒå‚æ•°
        bag_size: int = 16,
        set_size: int = 16, # Alias for bag_size
        mask_ratio: float = 0.0,
        mask_strategy: str = "random",
    ):
        """
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ¯ä¸ª batch çš„ bag æ•°é‡
            num_workers: DataLoader çš„ worker æ•°é‡
            pin_memory: æ˜¯å¦å°†æ•°æ®é”åœ¨å†…å­˜ä¸­
            io_chunk_size: TileDB è¯»å–æ—¶çš„ chunk å¤§å°
            prefetch_factor: æ¯ä¸ª worker é¢„åŠ è½½çš„ batch æ•°é‡
            persistent_workers: æ˜¯å¦ä¿æŒ workers å­˜æ´»
            split_label_train: è®­ç»ƒé›†æ ‡ç­¾
            split_label_val: éªŒè¯é›†æ ‡ç­¾
            shard_assignment: æ™ºèƒ½è´Ÿè½½å‡è¡¡çš„ shard åˆ†é…æ–¹æ¡ˆ
            bag_size: æ¯ä¸ªå¾®ç¯å¢ƒ bag ä¸­çš„ç»†èƒæ•°é‡
            set_size: bag_size çš„åˆ«åï¼Œä¼˜å…ˆä½¿ç”¨ bag_size (config ä¸­å¯èƒ½ä¸¤ä¸ªéƒ½æœ‰)
            mask_ratio: Masked AE çš„æ©ç æ¯”ä¾‹
            mask_strategy: æ©ç ç­–ç•¥
        """
        super().__init__()
        self.save_hyperparameters(logger=False)
        
        # Resolve bag_size / set_size alias
        # If set_size passed but bag_size is default, prefer set_size.
        # However, Hydra instantiates with whatever is in config.
        # Usually 'set_size' is the parameter we want to use.
        if set_size != 16 and bag_size == 16:
             self.hparams.bag_size = set_size
        # Or just sync them
        self.hparams.bag_size = set_size # Enforce set_size as primary if passed

        self.data_train: Optional[SomaNeighborhoodDataset] = None
        self.data_val: Optional[SomaNeighborhoodDataset] = None
        self._cached_sub_uris: Optional[list] = None

    def setup(self, stage: Optional[str] = None):
        """è®¾ç½®æ•°æ®é›†"""
        if not self.data_train and not self.data_val:
            # é¢„æ‰«æ Shards
            if self._cached_sub_uris is None:
                print(f"ğŸ” [NeighborhoodDataModule] Pre-scanning shards in {self.hparams.data_dir}...")
                self._cached_sub_uris = sorted([
                    os.path.join(self.hparams.data_dir, d)
                    for d in os.listdir(self.hparams.data_dir)
                    if os.path.isdir(os.path.join(self.hparams.data_dir, d))
                ])
                print(f"âœ… [NeighborhoodDataModule] Found {len(self._cached_sub_uris)} shards")

            # è®­ç»ƒé›†
            self.data_train = SomaNeighborhoodDataset(
                root_dir=self.hparams.data_dir,
                split_label=self.hparams.split_label_train,
                bag_size=self.hparams.bag_size,
                batch_size=self.hparams.batch_size,
                io_chunk_size=self.hparams.io_chunk_size,
                preloaded_sub_uris=self._cached_sub_uris,
                shard_assignment=self.hparams.shard_assignment,
                mask_ratio=self.hparams.mask_ratio,
                mask_strategy=self.hparams.mask_strategy,
            )

            # éªŒè¯é›†
            self.data_val = SomaNeighborhoodDataset(
                root_dir=self.hparams.data_dir,
                split_label=self.hparams.split_label_val,
                bag_size=self.hparams.bag_size,
                batch_size=self.hparams.batch_size,
                io_chunk_size=self.hparams.io_chunk_size,
                preloaded_sub_uris=self._cached_sub_uris,
                shard_assignment=None,  # éªŒè¯é›†ä¸éœ€è¦è´Ÿè½½å‡è¡¡
                mask_ratio=self.hparams.mask_ratio,
                mask_strategy=self.hparams.mask_strategy,
            )

    def train_dataloader(self):
        """è¿”å›è®­ç»ƒé›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_train,
            batch_size=None,  # Dataset å·²ç»å¤„ç†äº† batching
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers,
        )

    def val_dataloader(self):
        """è¿”å›éªŒè¯é›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_val,
            batch_size=None,
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers,
        )
