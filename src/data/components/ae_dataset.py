import torch
from torch.utils.data import IterableDataset
import tiledbsoma
import numpy as np
import math
import os
import random
import gc

class SomaCollectionDataset(IterableDataset):
    def __init__(self, root_dir, split_label=0, io_chunk_size=16384, batch_size=256, measurement_name="RNA", preloaded_sub_uris=None, shard_assignment=None):
        self.root_dir = root_dir
        self.split_label = split_label
        self.io_chunk_size = io_chunk_size
        self.batch_size = batch_size
        self.measurement_name = measurement_name
        self._n_vars = None  # å»¶è¿ŸåŠ è½½
        self.shard_assignment = shard_assignment  # æ™ºèƒ½è´Ÿè½½å‡è¡¡æ–¹æ¡ˆ
        
        # [å…³é”®ä¼˜åŒ–] å¦‚æœ DataModule æä¾›äº†é¢„æ‰«æçš„åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
        if preloaded_sub_uris is not None:
            self._sub_uris = preloaded_sub_uris
        else:
            self._sub_uris = None  # å»¶è¿ŸåŠ è½½ï¼ˆå‘åå…¼å®¹ï¼‰
        
        if not os.path.exists(root_dir):
             raise ValueError(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {root_dir}")
    
    @property
    def sub_uris(self):
        """å»¶è¿ŸåŠ è½½ Shards åˆ—è¡¨ï¼ˆå¦‚æœ DataModule æ²¡æœ‰é¢„æ‰«æï¼‰"""
        if self._sub_uris is None:
            # å…¼å®¹æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰é¢„åŠ è½½ï¼Œåˆ™ç”± Worker è‡ªå·±æ‰«æ
            self._sub_uris = sorted([
                os.path.join(self.root_dir, d) 
                for d in os.listdir(self.root_dir) 
                if os.path.isdir(os.path.join(self.root_dir, d))
            ])
            
            if len(self._sub_uris) == 0:
                raise ValueError(f"âŒ è·¯å¾„ {self.root_dir} ä¸‹æ²¡æœ‰å‘ç°å­æ–‡ä»¶å¤¹ï¼")
        
        return self._sub_uris
    
    @property
    def n_vars(self):
        """å»¶è¿ŸåŠ è½½ç‰¹å¾ç»´åº¦ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡çœŸæ­£éœ€è¦æ—¶æ‰è¯»å–å…ƒæ•°æ®ï¼‰"""
        if self._n_vars is None:
            tmp_ctx = tiledbsoma.SOMATileDBContext()
            try:
                with tiledbsoma.Experiment.open(self.sub_uris[0], context=tmp_ctx) as exp:
                    self._n_vars = exp.ms[self.measurement_name].var.count
            except Exception:
                if len(self.sub_uris) > 1:
                    with tiledbsoma.Experiment.open(self.sub_uris[1], context=tmp_ctx) as exp:
                        self._n_vars = exp.ms[self.measurement_name].var.count
                else:
                    raise
        
        return self._n_vars

    def _get_context(self):
        return tiledbsoma.SOMATileDBContext(tiledb_config={
            "py.init_buffer_bytes": 512 * 1024**2,
            "sm.memory_budget": 4 * 1024**3,
        })

    def __iter__(self):
        # 1. è·å– DDP å’Œ Worker ä¿¡æ¯
        if torch.distributed.is_initialized():
            rank = torch.distributed.get_rank()
            world_size = torch.distributed.get_world_size()
        else:
            rank = 0
            world_size = 1
            
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is None:
            worker_id = 0
            num_workers = 1
        else:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
        
        # 2. è®¡ç®—å…¨å±€ Worker IDï¼ˆè·¨ GPUï¼‰
        global_worker_id = rank * num_workers + worker_id
        
        # 3. é€‰æ‹©åˆ†ç‰‡ç­–ç•¥
        if self.shard_assignment is not None:
            # ç­–ç•¥ Aï¼šä½¿ç”¨æ™ºèƒ½è´Ÿè½½å‡è¡¡æ–¹æ¡ˆï¼ˆæ¨èï¼‰
            assigned_shard_names = self.shard_assignment.get(str(global_worker_id), [])
            # å°† shard åç§°è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
            shard_name_to_uri = {os.path.basename(uri): uri for uri in self.sub_uris}
            my_worker_uris = [shard_name_to_uri[name] for name in assigned_shard_names if name in shard_name_to_uri]
        else:
            # ç­–ç•¥ Bï¼šç®€å•è½®è¯¢ï¼ˆå‘åå…¼å®¹ï¼‰
            total_workers = world_size * num_workers
            global_uris = sorted(self.sub_uris)
            my_worker_uris = global_uris[global_worker_id::total_workers]

        if len(my_worker_uris) == 0:
            return

        # 4. æ‰“ä¹±å¤„ç†é¡ºåºï¼ˆæ¯ä¸ª epoch éƒ½ä¸ä¸€æ ·ï¼‰
        random.shuffle(my_worker_uris)
        
        ctx = self._get_context()
        
        # å¤§å—å†…å­˜æ± ï¼ˆå¤ç”¨ï¼‰
        dense_buffer = np.zeros((self.io_chunk_size, self.n_vars), dtype=np.float32)

        try:
            for uri in my_worker_uris:
                try:
                    with tiledbsoma.Experiment.open(uri, context=ctx) as exp:
                        try:
                            query = exp.obs.read(
                                value_filter=f"split_label == {self.split_label}",
                                column_names=["soma_joinid"]
                            ).concat()
                            chunk_ids = query["soma_joinid"].to_numpy().copy()
                        except Exception:
                            continue 
                        
                        if len(chunk_ids) == 0: continue
                        np.random.shuffle(chunk_ids)
                        
                        x_uri = os.path.join(uri, "ms", self.measurement_name, "X", "data")
                        
                        with tiledbsoma.open(x_uri, mode='r', context=ctx) as X:
                            for i in range(0, len(chunk_ids), self.io_chunk_size):
                                sub_ids = chunk_ids[i : i + self.io_chunk_size]
                                current_len = len(sub_ids)
                                read_ids = np.sort(sub_ids)
                                
                                data = X.read(coords=(read_ids, slice(None))).tables().concat()
                                
                                row_indices = data["soma_dim_0"].to_numpy()
                                col_indices = data["soma_dim_1"].to_numpy()
                                values = data["soma_data"].to_numpy()
                                
                                local_rows = np.searchsorted(read_ids, row_indices)
                                
                                # --- ğŸ”¥ ä¿®å¤ç‚¹åœ¨è¿™é‡Œ ğŸ”¥ ---
                                # å¿…é¡»å…ˆå®šä¹‰ active_buffer æ˜¯ dense_buffer çš„ä¸€ä¸ªåˆ‡ç‰‡
                                active_buffer = dense_buffer[:current_len]
                                
                                # ç„¶åæ‰èƒ½æ¸…é›¶å’Œèµ‹å€¼
                                active_buffer.fill(0)
                                active_buffer[local_rows, col_indices] = values
                                
                                perm = np.random.permutation(current_len)
                                num_batches = (current_len + self.batch_size - 1) // self.batch_size
                                
                                for b in range(num_batches):
                                    start_idx = b * self.batch_size
                                    end_idx = min(start_idx + self.batch_size, current_len)
                                    batch_perm_idx = perm[start_idx:end_idx]
                                    
                                    # [CRITICAL FIX] æ£€æŸ¥æœ€åä¸€ä¸ª batch æ˜¯å¦å¤ªå°
                                    # å¦‚æœå¤ªå° (æ¯”å¦‚ 1)ï¼ŒBatchNorm ä¼šå´©æºƒ
                                    if len(batch_perm_idx) <= 1:
                                        continue
                                    
                                    out_tensor = torch.from_numpy(active_buffer[batch_perm_idx].copy())
                                    out_labels = torch.zeros(len(out_tensor), dtype=torch.long)
                                    
                                    yield out_tensor, out_labels
                                    
                except Exception as e:
                    print(f"âš ï¸ Error processing {os.path.basename(uri)}: {e}")
                    continue
                    
        finally:
            del dense_buffer
            del ctx
            gc.collect()
