import torch
from torch.utils.data import IterableDataset, DataLoader
import tiledbsoma
import numpy as np
import os
import random
import gc
from typing import Optional, Dict, List
import scipy.sparse

class FlowSomaDataset(IterableDataset):
    """
    ä¸“é—¨ç”¨äº Flow Matching çš„æ•°æ®é›†ã€‚
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. Latent Space: è¯»å–æå–å¥½çš„ Dense Latent Code (latent_key="X_latent")
    2. Raw Space: è¯»å–åŸå§‹ Sparse Gene Expression (latent_key="ms/RNA/X/data")ï¼Œå¹¶è½¬ä¸º Denseã€‚
    """
    def __init__(
        self, 
        root_dir: str, 
        split_label: int = 0, 
        io_chunk_size: int = 4096, 
        batch_size: int = 256,
        latent_key: str = "X_latent",
        condition_keys: Optional[Dict[str, str]] = None
    ):
        self.root_dir = root_dir
        self.split_label = split_label
        self.io_chunk_size = io_chunk_size
        self.batch_size = batch_size
        self.latent_key = latent_key
        
        # é»˜è®¤æ¡ä»¶é”®åæ˜ å°„
        self.condition_keys = condition_keys or {
            "time": "unified_time",
            "tissue": "tissue_code",
            "celltype": "celltype_code"
        }
        
        if not os.path.exists(root_dir):
             raise ValueError(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {root_dir}")

        self.sub_uris = sorted([
            os.path.join(root_dir, d) 
            for d in os.listdir(root_dir) 
            if os.path.isdir(os.path.join(root_dir, d))
        ])
        print(f"ğŸŒ [FlowDataset] æ‰«æ Shards: {len(self.sub_uris)} ä¸ª | Layer: {latent_key}")

    def _get_context(self):
        return tiledbsoma.SOMATileDBContext()

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is None:
            my_uris = self.sub_uris.copy()
        else:
            # ç®€å•çš„åˆ†ç‰‡é€»è¾‘
            import math
            per_worker = int(math.ceil(len(self.sub_uris) / worker_info.num_workers))
            start = worker_info.id * per_worker
            end = min(start + per_worker, len(self.sub_uris))
            my_uris = self.sub_uris[start:end].copy()

        random.shuffle(my_uris)
        ctx = self._get_context()
        
        # æå–å…³é”®åˆ—å
        time_col = self.condition_keys.get("time", "unified_time")
        tissue_col = self.condition_keys.get("tissue", "tissue_code")
        celltype_col = self.condition_keys.get("celltype", "celltype_code")

        for uri in my_uris:
            try:
                with tiledbsoma.Experiment.open(uri, context=ctx) as exp:
                    # 1. è¯»å– Metadata
                    try:
                        obs_df = exp.obs.read(
                            value_filter=f"split_label == {self.split_label}",
                            column_names=["soma_joinid", "next_cell_idx", "is_valid_transition", 
                                          time_col, tissue_col, celltype_col]
                        ).concat().to_pandas()
                    except Exception as e:
                        # å®¹é”™ï¼šå¯èƒ½æŸäº›å­—æ®µä¸å­˜åœ¨
                        print(f"âš ï¸ Error reading obs from {uri}: {e}")
                        continue
                    
                    if len(obs_df) == 0: continue
                    
                    valid_df = obs_df[obs_df['is_valid_transition'] == 1]
                    if len(valid_df) == 0: continue
                    
                    # 2. åŠ¨æ€å®šä½ Data Array
                    x_arr = None
                    if self.latent_key in exp:
                        x_arr = exp[self.latent_key]
                    elif '/' in self.latent_key:
                        # è§£æè·¯å¾„: ms/RNA/X/data
                        parts = self.latent_key.split('/')
                        curr = exp
                        for p in parts:
                            curr = curr[p]
                        x_arr = curr
                    
                    if x_arr is None:
                         print(f"âš ï¸ Could not find array {self.latent_key} in {uri}")
                         continue

                    # 3. è¯»å–æ•°æ® (å¤„ç† Sparse vs Dense)
                    # å¦‚æœæ˜¯ Raw Data (Sparse)ï¼Œå¿…é¡»è½¬ä¸º Dense
                    is_sparse = x_arr.soma_type == "SOMASparseNDArray"
                    
                    if is_sparse:
                        # Sparse è¯»å– -> COO -> Dense
                        # æ³¨æ„ï¼šå¦‚æœ Shard å¾ˆå¤§ä¸” input_dim å¾ˆå¤§ (20k)ï¼Œè¿™é‡Œå†…å­˜ä¼šæš´æ¶¨
                        # Shard å»ºè®®æ§åˆ¶åœ¨ 2000-4000 ç»†èƒ
                        table = x_arr.read().tables().concat()
                        if len(table) == 0: continue
                        
                        # è·å–ç»´åº¦ (Shards åº”è¯¥çŸ¥é“å…¨å±€ shape å—ï¼Ÿæˆ–è€…åªç”¨ max index)
                        # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œè¯»å–åˆ°çš„ shape åªæ˜¯å½“å‰ shard çš„æœ€å¤§å€¼
                        # ä½† DiT éœ€è¦å›ºå®šçš„ input_dimã€‚
                        # æˆ‘ä»¬å‡è®¾ x_arr.shape[1] æ˜¯æ­£ç¡®çš„ feature æ•°é‡
                        n_vars = x_arr.shape[1] 
                        n_cells_shard = obs_df.index.max() + 1 # å‡è®¾ soma_joinid æ˜¯å±€éƒ¨çš„ 0..N-1
                        
                        # æ„å»º CSR
                        rows = table['soma_dim_0'].to_numpy()
                        cols = table['soma_dim_1'].to_numpy()
                        data = table['soma_data'].to_numpy()
                        
                        # ç¡®ä¿ rows å¯¹åº”åˆ° 0..N-1
                        # è¿™é‡Œçš„ obs_df.index å°±æ˜¯ soma_joinid
                        # å¦‚æœæ˜¯ Sparseï¼Œæˆ‘ä»¬å…ˆæ„å»ºå¤§çŸ©é˜µï¼Œå†åˆ‡ç‰‡
                        full_matrix_sparse = scipy.sparse.csr_matrix(
                            (data, (rows, cols)), 
                            shape=(n_cells_shard, n_vars),
                            dtype=np.float32
                        )
                        full_latents = full_matrix_sparse.toarray()
                        
                    else:
                        # Dense è¯»å– (Latent)
                        full_latents = x_arr.read().to_numpy()

                    full_latents_t = torch.from_numpy(full_latents)
                    
                    # 4. æ„å»º Batch
                    indices = valid_df.index.values # Source Indices
                    np.random.shuffle(indices)
                    
                    num_samples = len(indices)
                    num_batches = (num_samples + self.batch_size - 1) // self.batch_size
                    
                    for b in range(num_batches):
                        batch_idx = indices[b*self.batch_size : (b+1)*self.batch_size]
                        
                        if len(batch_idx) <= 1: continue 
                        
                        # è·å–æ•°æ®
                        x_curr = full_latents_t[batch_idx]
                        
                        next_indices = obs_df.loc[batch_idx, 'next_cell_idx'].values.astype(int)
                        x_next = full_latents_t[next_indices]
                        
                        times = torch.tensor(obs_df.loc[batch_idx, time_col].values, dtype=torch.float32)
                        next_times = torch.tensor(obs_df.loc[next_indices, time_col].values, dtype=torch.float32)
                        dt = next_times - times
                        
                        tissues = torch.tensor(obs_df.loc[batch_idx, tissue_col].values, dtype=torch.long)
                        celltypes = torch.tensor(obs_df.loc[batch_idx, celltype_col].values, dtype=torch.long)
                        
                        yield {
                            'x_curr': x_curr,
                            'x_next': x_next,
                            'cond_meta': {
                                'time': times,
                                'dt': dt,
                                'tissue': tissues,
                                'celltype': celltypes
                            }
                        }

            except Exception as e:
                print(f"âš ï¸ Error processing {os.path.basename(uri)}: {e}")
                continue
