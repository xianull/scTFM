import argparse
import logging
from pathlib import Path
import numpy as np
import pandas as pd
import scanpy as sc
import tiledb
from tqdm import tqdm
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import json
import gc
import scipy.sparse as sp
from typing import List, Dict, Optional, Tuple
import threading
import queue
import time

import shutil
import subprocess

# å¿½ç•¥ Scanpy çš„ä¸€äº› FutureWarning
warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_gene_vocab(vocab_path: str) -> List[str]:
    """åŠ è½½åŸºå› è¯è¡¨"""
    path = Path(vocab_path)
    if not path.exists():
        raise FileNotFoundError(f"Vocabulary file not found: {path}")
    with open(path, 'r') as f:
        genes = [line.strip() for line in f if line.strip()]
    return genes

def process_h5ad_vectorized(args) -> Optional[Dict]:
    """
    Worker å‡½æ•°ï¼šå¤„ç†å•ä¸ª h5ad æ–‡ä»¶
    """
    # ã€ä¼˜åŒ–ã€‘æ¥æ”¶é¢„æ„å»ºå¥½çš„ gene_mapï¼Œè€Œä¸æ˜¯ list
    file_path, target_gene_map, target_genes_list, min_genes, target_sum, is_ood_flag = args
    
    try:
        # 1. è¯»å–æ•°æ®
        adata = sc.read_h5ad(file_path)

        # 2. ç»Ÿä¸€åŸºå› åä¸ºç´¢å¼•
        if "gene_symbols" in adata.var.columns:
            adata.var_names = adata.var["gene_symbols"].astype(str)
        adata.var_names_make_unique()

        # 3. è¿‡æ»¤ç»†èƒ
        if min_genes > 0:
            sc.pp.filter_cells(adata, min_genes=min_genes)

        if adata.shape[0] == 0:
            return None

        # 4. å½’ä¸€åŒ– (æ³¨æ„ï¼šè¿™é‡Œä¼šæ”¹å˜æ•°æ®ä¸º log1p)
        sc.pp.normalize_total(adata, target_sum=target_sum)
        sc.pp.log1p(adata)

        # --- æ ¸å¿ƒå‘é‡åŒ–é€»è¾‘ ---
        
        # A. æ‰¾åˆ°äº¤é›†åŸºå›  (åˆ©ç”¨ set è½¬æ¢ list åŠ é€Ÿ isin)
        mask = np.isin(adata.var_names, target_genes_list)
        
        # B. åˆ‡ç‰‡
        adata_sub = adata[:, mask]
        
        if adata_sub.shape[1] == 0:
            return None 
            
        # C. è½¬ COO
        X_coo = adata_sub.X.tocoo()
        
        # D. ç´¢å¼•é‡æ˜ å°„
        sub_gene_names = adata_sub.var_names
        
        # ä½¿ç”¨ä¼ å…¥çš„ map è¿›è¡Œæ˜ å°„
        local_to_global = np.array([target_gene_map[g] for g in sub_gene_names], dtype=np.int64)
        new_gene_indices = local_to_global[X_coo.col]
        
        # æ„å»ºç»“æœ
        res = {
            'n_cells': adata.shape[0],
            'row_indices': X_coo.row.astype(np.int64),
            'col_indices': new_gene_indices,
            'values': X_coo.data.astype(np.float32),
            'is_ood': is_ood_flag,
            'file_path': str(file_path)
        }
        
        # ã€å®‰å…¨ã€‘ä¸»åŠ¨é‡Šæ”¾å¤§å¯¹è±¡å†…å­˜
        del adata, adata_sub, X_coo
        gc.collect()
        
        return res

    except Exception as e:
        logger.error(f"Error processing {file_path}: {e}")
        return None

def init_tiledb_array(output_dir: Path, n_genes: int):
    tiledb_path = output_dir / "all_data"
    
    if tiledb_path.exists():
        import shutil
        logger.warning(f"Output path exists, cleaning up: {tiledb_path}")
        shutil.rmtree(tiledb_path)
    tiledb_path.mkdir(parents=True)
    
    # ç“¦ç‰‡å¤§å° 4096 (scimilarity æ¨è: 2048-4096)
    # é¿å… GPFS ä¸Šçš„è¯»æ”¾å¤§
    tile_extent = 4096
    # é˜²æ­¢ int64 æº¢å‡º
    max_domain = np.iinfo(np.int64).max - tile_extent - 1000
    
    # é»˜è®¤å‹ç¼©è¿‡æ»¤å™¨
    filters = [tiledb.ZstdFilter(level=4)]
    
    # 1. Counts Schema (Sparse)
    counts_uri = str(tiledb_path / "counts")
    dom = tiledb.Domain(
        tiledb.Dim(name="cell_index", domain=(0, max_domain), tile=tile_extent, dtype=np.int64),
        tiledb.Dim(name="gene_index", domain=(0, n_genes - 1), tile=n_genes, dtype=np.int64),
    )
    schema = tiledb.ArraySchema(
        domain=dom, sparse=True, 
        attrs=[tiledb.Attr(name="data", dtype=np.float32, filters=filters)], 
        allows_duplicates=False,
        coords_filters=filters,
        offsets_filters=filters
    )
    tiledb.Array.create(counts_uri, schema)
    
    # 2. Metadata Schema (Dense!)
    # ä¼˜åŒ–ï¼šä½¿ç”¨ Dense Array å­˜å‚¨ Metadataï¼Œæ”¯æŒ O(1) éšæœºè®¿é—®
    meta_uri = str(tiledb_path / "cell_metadata")
    dom_meta = tiledb.Domain(
        tiledb.Dim(name="cell_index", domain=(0, max_domain), tile=tile_extent, dtype=np.int64)
    )
    schema_meta = tiledb.ArraySchema(
        domain=dom_meta, sparse=False, # Changed to Dense
        attrs=[
            tiledb.Attr(name="is_ood", dtype=np.int8, filters=filters),
            tiledb.Attr(name="file_source", dtype='ascii', var=True, filters=filters) 
        ]
    )
    tiledb.Array.create(meta_uri, schema_meta)
    
    return tiledb_path

class AsyncBatchWriter:
    def __init__(self, tiledb_path: Path, batch_size: int = 500000):
        self.counts_uri = str(tiledb_path / "counts")
        self.meta_uri = str(tiledb_path / "cell_metadata")
        self.batch_size = batch_size
        self.global_cell_offset = 0
        
        self.current_buffer = self._init_buffer()
        self.current_count = 0
        
        # ã€é‡è¦ä¿®å¤ã€‘è®¾ç½® maxsize é˜²æ­¢å†…å­˜çˆ†ç‚¸ (èƒŒå‹æœºåˆ¶)
        # å…è®¸é˜Ÿåˆ—é‡Œå­˜ 3 ä¸ª batchï¼Œå¦‚æœæ»¡äº†ï¼Œä¸»è¿›ç¨‹çš„ add() ä¼šé˜»å¡ç­‰å¾…
        self.write_queue = queue.Queue(maxsize=3)
        
        self.is_running = True
        self.writer_thread = threading.Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()
        self.write_error = None

    def _init_buffer(self):
        return {
            'rows': [], 'cols': [], 'vals': [],
            'meta_indices': [], 'meta_ood': [], 'meta_src': []
        }

    def add(self, result: Dict):
        if self.write_error: raise self.write_error
        if not result: return
            
        n_cells = result['n_cells']
        global_rows = result['row_indices'] + self.global_cell_offset
        
        self.current_buffer['rows'].append(global_rows)
        self.current_buffer['cols'].append(result['col_indices'])
        self.current_buffer['vals'].append(result['values'])
        self.current_buffer['meta_indices'].append(np.arange(self.global_cell_offset, self.global_cell_offset + n_cells))
        self.current_buffer['meta_ood'].append(np.full(n_cells, result['is_ood'], dtype=np.int8))
        self.current_buffer['meta_src'].extend([result['file_path']] * n_cells)
        
        self.global_cell_offset += n_cells
        self.current_count += n_cells
        
        if self.current_count >= self.batch_size:
            self._push_to_queue()

    def _push_to_queue(self):
        if self.current_count == 0: return
        logger.info(f"  [Main] Batch full ({self.current_count} cells). Pushing to queue (Size: {self.write_queue.qsize()})...")
        
        task = (self.current_buffer, self.current_count)
        # put é»˜è®¤æ˜¯é˜»å¡çš„ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œè¿™é‡Œä¼šåœä¸‹ç­‰å¾…ï¼Œä¿æŠ¤å†…å­˜
        self.write_queue.put(task)
        
        self.current_buffer = self._init_buffer()
        self.current_count = 0

    def _writer_loop(self):
        while self.is_running or not self.write_queue.empty():
            try:
                try:
                    buffer, count = self.write_queue.get(timeout=1)
                except queue.Empty:
                    continue
                
                # --- [æ–°å¢] è¯¦ç»†åˆ†æ®µè®¡æ—¶ ---
                logger.info(f"  [Debug] Start writing batch of {count} cells...")
                t_start = time.time()
                
                # 1. å†…å­˜æ‹¼æ¥ (æ£€æµ‹æ˜¯å¦çˆ†å†…å­˜/Swap)
                all_rows = np.concatenate(buffer['rows'])
                all_cols = np.concatenate(buffer['cols'])
                all_vals = np.concatenate(buffer['vals'])
                
                t_concat = time.time()
                logger.info(f"  [Debug] Step 1: Numpy Concat used {t_concat - t_start:.2f}s") # å¦‚æœè¿™é‡Œæ…¢ï¼Œè¯´æ˜å†…å­˜çˆ†äº†

                # 2. TileDB ä¸Šä¸‹æ–‡é…ç½®
                cfg = tiledb.Config({
                    "sm.compute_concurrency_level": "100",
                    "sm.io_concurrency_level": "16",
                    "vfs.file.enable_filelocks": "false", # ç¡®ä¿é”å·²å…³
                })
                ctx = tiledb.Ctx(cfg)
                
                # 3. å†™ Counts (æ£€æµ‹ç¡¬ç›˜é€Ÿåº¦)
                with tiledb.open(self.counts_uri, 'w', ctx=ctx) as arr:
                    arr[all_rows, all_cols] = all_vals
                
                t_write_counts = time.time()
                logger.info(f"  [Debug] Step 2: Write Counts Disk IO used {t_write_counts - t_concat:.2f}s") # å¦‚æœè¿™é‡Œæ…¢ï¼Œè¯´æ˜æ˜¯ GPFS çš„é—®é¢˜

                # 4. å†™ Metadata (Dense Optimized)
                all_meta_indices = np.concatenate(buffer['meta_indices'])
                all_meta_ood = np.concatenate(buffer['meta_ood'])
                
                # Dense Array å†™å…¥ä¼˜åŒ–ï¼šä½¿ç”¨åˆ‡ç‰‡èµ‹å€¼
                start_idx = int(all_meta_indices[0])
                end_idx = int(all_meta_indices[-1]) + 1
                
                with tiledb.open(self.meta_uri, 'w', ctx=ctx) as arr:
                    arr[start_idx:end_idx] = {
                        'is_ood': all_meta_ood,
                        'file_source': np.array(buffer['meta_src'], dtype=object)
                    }
                
                # 5. æ¸…ç†
                del buffer, all_rows, all_cols, all_vals
                gc.collect()
                
                logger.info(f"  [Async] Total Batch Time: {time.time()-t_start:.1f}s.")
                self.write_queue.task_done()
                
            except Exception as e:
                logger.error(f"Async Writer Crashed: {e}")
                self.write_error = e
                break

    def finish(self):
        self._push_to_queue()
        self.is_running = False
        self.writer_thread.join()
        if self.write_error: raise self.write_error

def consolidate_arrays(tiledb_path: Path):
    """åˆå¹¶ TileDB ç¢ç‰‡ä»¥ä¼˜åŒ–è¯»å–æ€§èƒ½"""
    logger.info("Starting consolidation...")
    
    # 1. Consolidate Counts
    counts_uri = str(tiledb_path / "counts")
    tiledb.consolidate(counts_uri)
    tiledb.vacuum(counts_uri)
    
    # 2. Consolidate Metadata
    meta_uri = str(tiledb_path / "cell_metadata")
    tiledb.consolidate(meta_uri)
    tiledb.vacuum(meta_uri)
    
    logger.info("Consolidation complete.")

def main():
    parser = argparse.ArgumentParser(description="Efficient TileDB Converter (In-Memory Fast Track)")
    parser.add_argument("--csv_path", type=str, default="data/assets/ae_data_info_1000.csv")
    parser.add_argument("--vocab_path", type=str, default="data/assets/gene_order.tsv")
    
    # ã€æœ€ç»ˆç›®çš„åœ°ã€‘GPFS è·¯å¾„
    parser.add_argument("--final_output_dir", type=str, 
                        default="/gpfs/hybrid/data/downloads/gcloud/arc-scbasecount/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/tiledb_10m")
    
    parser.add_argument("--min_genes", type=int, default=200)
    parser.add_argument("--target_sum", type=float, default=1e4)
    parser.add_argument("--num_workers", type=int, default=64) # ä¿æŒ 64 ä»¥é˜² OOM
    # ä¼˜åŒ–ï¼šè®¾ç½®ä¸º Tile Extent (4096) çš„æ•´æ•°å€ï¼Œç¡®ä¿ Dense Array å†™å…¥å¯¹é½
    # 4096 * 128 = 524288
    parser.add_argument("--batch_size", type=int, default=524288)
    parser.add_argument("--max_files", type=int, default=1000)
    
    args = parser.parse_args()
    
    # --- 1. è®¾ç½®æé€Ÿå†…å­˜è·¯å¾„ ---
    # åˆ©ç”¨ Linux çš„ /dev/shm (Shared Memory)ï¼Œé€Ÿåº¦æ¯” NVMe å¿« 10 å€ï¼Œä¸”æ²¡æœ‰æ–‡ä»¶é”å»¶è¿Ÿ
    shm_path = Path("/dev/shm/tiledb_fast_buffer")
    final_path = Path(args.final_output_dir)
    
    logger.info("="*60)
    logger.info(f"ğŸš€ SPEED MODE ACTIVATED")
    logger.info(f"1. Working Directory (RAM): {shm_path}")
    logger.info(f"2. Final Destination (SSD): {final_path}")
    logger.info("="*60)
    
    # æ¸…ç†æ—§çš„å†…å­˜ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if shm_path.exists():
        logger.warning(f"Cleaning up previous buffer at {shm_path}...")
        shutil.rmtree(shm_path)
    
    # 2. å‡†å¤‡å·¥ä½œ
    target_genes = load_gene_vocab(args.vocab_path)
    target_gene_map = {g: i for i, g in enumerate(target_genes)}
    
    info_df = pd.read_csv(args.csv_path)
    if args.max_files > 0:
        info_df = info_df.head(args.max_files)

    # 3. åˆå§‹åŒ– TileDB (åœ¨å†…å­˜ç›˜ä¸Šï¼)
    tiledb_path = init_tiledb_array(shm_path, len(target_genes))
    
    # å†™å…¥åŸºå› æ³¨é‡Š
    gene_annot_uri = str(tiledb_path / "gene_annotation")
    tiledb.Array.create(gene_annot_uri, tiledb.ArraySchema(
        domain=tiledb.Domain(tiledb.Dim(name="gene_index", domain=(0, len(target_genes)-1), tile=len(target_genes), dtype=np.int64)),
        sparse=False,
        attrs=[tiledb.Attr(name="gene_symbol", dtype='ascii', var=True)]
    ))
    with tiledb.open(gene_annot_uri, 'w') as arr:
        arr[:] = {'gene_symbol': np.array(target_genes, dtype=object)}

    # 4. å‡†å¤‡ä»»åŠ¡
    tasks = []
    for _, row in info_df.iterrows():
        is_ood = int(row.get('full_validation_dataset', 0))
        tasks.append((
            row['file_path'], target_gene_map, target_genes,
            args.min_genes, args.target_sum, is_ood
        ))

    # 5. æ‰§è¡Œ (å†™å…¥å†…å­˜ç›˜)
    writer = AsyncBatchWriter(tiledb_path, batch_size=args.batch_size)
    logger.info(f"Starting processing {len(tasks)} files...")
    
    with ProcessPoolExecutor(max_workers=args.num_workers) as executor:
        futures = {executor.submit(process_h5ad_vectorized, task): task[0] for task in tasks}
        for future in tqdm(as_completed(futures), total=len(tasks), desc="Processing & Writing (RAM)"):
            try:
                result = future.result()
                if result: writer.add(result)
            except Exception as e:
                logger.error(f"Failed: {e}")

    writer.finish()
    
    # --- [æ–°å¢] åˆå¹¶ç¢ç‰‡ (Consolidation) ---
    # è¿™ä¸€æ­¥å¯¹äº GPFS æ€§èƒ½è‡³å…³é‡è¦ï¼Œé¿å…æ•°åƒä¸ªå°æ–‡ä»¶
    consolidate_arrays(tiledb_path)
    
    # ä¿å­˜ Metadata
    metadata = {
        'total_cells': writer.global_cell_offset,
        'n_genes': len(target_genes),
        'storage_path': str(final_path) # è¿™é‡Œè®°å½•æœ€ç»ˆè·¯å¾„
    }
    with open(tiledb_path / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    # --- 6. æœ€ç»ˆæ¬è¿ (RAM -> GPFS) ---
    logger.info("="*60)
    logger.info("Processing Complete. Moving data from RAM to GPFS...")
    logger.info("This may take a while, but it's much faster than direct writing.")
    
    # ç¡®ä¿ç›®æ ‡çˆ¶ç›®å½•å­˜åœ¨
    final_path.parent.mkdir(parents=True, exist_ok=True)
    if final_path.exists():
        shutil.rmtree(final_path) # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼ˆæ…ç”¨ï¼Œæˆ–è€…æ”¹åï¼‰
    
    # ä½¿ç”¨ rsync è¿›è¡Œæ¬è¿ (æ¯” shutil æ›´ç¨³å¥ï¼Œæ˜¾ç¤ºè¿›åº¦)
    try:
        cmd = ["rsync", "-avP", str(shm_path) + "/", str(final_path) + "/"]
        subprocess.run(cmd, check=True)
        logger.info(f"âœ… SUCCESS! Data moved to: {final_path}")
        
        # æ¬è¿æˆåŠŸåï¼Œæ¸…ç†å†…å­˜
        shutil.rmtree(shm_path)
        logger.info("RAM buffer cleaned.")
        
    except Exception as e:
        logger.error(f"âŒ Error moving data: {e}")
        logger.error(f"âš ï¸ YOUR DATA IS STILL IN: {shm_path}. PLEASE MOVE IT MANUALLY!")

if __name__ == "__main__":
    multiprocessing.set_start_method('fork', force=True)
    main()