"""
é‡æ„ç‰ˆ TileDB é¢„å¤„ç†è„šæœ¬ï¼šæŒ‰å›ºå®šç»†èƒæ•°åˆ‡åˆ† Shard
è§£å†³åŸç‰ˆè„šæœ¬ä¸­ Shard å¤§å°ä¸å‡å¯¼è‡´çš„è®­ç»ƒè´Ÿè½½ä¸å‡è¡¡é—®é¢˜

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä¸å† 1 æ–‡ä»¶ = 1 Shardï¼Œè€Œæ˜¯ N ä¸ªç»†èƒ = 1 Shard
2. ç¡®ä¿æ¯ä¸ª Shard å¤§å°ä¸€è‡´ï¼ˆæœ€åä¸€ä¸ªå¯èƒ½ç•¥å°ï¼‰
3. å‡å°‘ Shard æ€»æ•°ï¼Œæå‡ I/O æ•ˆç‡
4. æ›´å¥½çš„ DDP è´Ÿè½½å‡è¡¡
"""

import multiprocessing
import os
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

import shutil
import pandas as pd
import numpy as np
import scanpy as sc
import anndata as ad
import scipy.sparse
import tiledbsoma
import tiledbsoma.io
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path

warnings.filterwarnings('ignore')

# ==================== é…ç½®å‚æ•° ====================
CSV_PATH = '/gpfs/flash/home/jcw/projects/research/cellTime/scTFM/data/ae_data_info.csv'
GENE_ORDER_PATH = "/gpfs/flash/home/jcw/projects/research/cellTime/scTFM/data/gene_order.tsv"
OUTPUT_BASE_URI = "/fast/data/scTFM/ae/tile_4000_fix"

CELLS_PER_SHARD = 4000  # æ¯ä¸ª Shard çš„ç›®æ ‡ç»†èƒæ•°
MAX_WORKERS = 16        # å¹¶è¡Œè¯»å–æ–‡ä»¶çš„è¿›ç¨‹æ•°

# ==================== å…¨å±€å˜é‡ ====================
global_target_genes = None
global_target_gene_map = None

def worker_init(gene_list):
    """å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½ç›®æ ‡åŸºå› åˆ—è¡¨"""
    global global_target_genes, global_target_gene_map
    global_target_genes = gene_list
    global_target_gene_map = {gene: i for i, gene in enumerate(gene_list)}

def load_and_process_one_file(file_path, is_full_val):
    """
    åŠ è½½å¹¶é¢„å¤„ç†å•ä¸ª h5ad æ–‡ä»¶
    è¿”å›å¤„ç†åçš„ AnnDataï¼ˆå·²å¯¹é½ã€å½’ä¸€åŒ–ã€log1pï¼‰
    """
    try:
        if not os.path.exists(file_path):
            return None, f"Missing: {file_path}"
        
        # 1. è¯»å–æ•°æ®
        adata = sc.read_h5ad(file_path)
        
        # 2. å‡†å¤‡å˜é‡å
        adata.var_names = adata.var['gene_symbols'].astype(str)
        adata.var_names_make_unique()
        
        # 3. è¿‡æ»¤ä½è´¨é‡ç»†èƒ
        sc.pp.filter_cells(adata, min_genes=200)
        
        if adata.n_obs == 0:
            return None, "Skipped (Low quality)"
        
        # 4. åŸºå› å¯¹é½
        target_genes = global_target_genes
        target_n_vars = len(target_genes)
        target_gene_map = global_target_gene_map
        
        common_genes = [g for g in adata.var_names if g in target_gene_map]
        
        if len(common_genes) == 0:
            new_X = scipy.sparse.csr_matrix((adata.n_obs, target_n_vars), dtype=np.float32)
            adata = ad.AnnData(X=new_X, obs=adata.obs)
            adata.var_names = target_genes
        else:
            adata = adata[:, common_genes].copy()
            
            if not scipy.sparse.isspmatrix_csr(adata.X):
                adata.X = adata.X.tocsr()
            
            current_col_to_target_col = np.array(
                [target_gene_map[g] for g in adata.var_names], 
                dtype=np.int32
            )
            new_indices = current_col_to_target_col[adata.X.indices]
            
            new_X = scipy.sparse.csr_matrix(
                (adata.X.data, new_indices, adata.X.indptr),
                shape=(adata.n_obs, target_n_vars)
            )
            new_X.sort_indices()
            
            adata = ad.AnnData(X=new_X, obs=adata.obs)
            adata.var_names = target_genes
        
        # 5. å½’ä¸€åŒ–å’Œ log1p
        sc.pp.normalize_total(adata, target_sum=1e4)
        sc.pp.log1p(adata)
        
        # 6. æ‰“æ ‡ç­¾
        if is_full_val == 1:
            adata.obs['split_label'] = 3  # Test OOD
        else:
            n_cells = adata.n_obs
            split_labels = np.random.choice(
                [0, 1, 2], 
                size=n_cells, 
                p=[0.9, 0.05, 0.05]
            )
            adata.obs['split_label'] = split_labels
        
        adata.obs['split_label'] = adata.obs['split_label'].astype(np.int32)
        
        # 7. ç¡®ä¿ float32
        if adata.X.dtype != np.float32:
            adata.X = adata.X.astype(np.float32)
        
        return adata, "Success"
    
    except Exception as e:
        return None, f"Error: {str(e)}"

def batch_load_files(df, max_workers=16):
    """
    å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶ï¼Œè¿”å› AnnData åˆ—è¡¨
    """
    print(f"ğŸ“‚ Loading {len(df)} files with {max_workers} workers...")
    
    tasks = [(row['file_path'], row['full_validation_dataset']) 
             for _, row in df.iterrows()]
    
    all_adatas = []
    stats = {"Success": 0, "Skipped": 0, "Errors": 0}
    
    with ProcessPoolExecutor(max_workers=max_workers, 
                             initializer=worker_init, 
                             initargs=(global_target_genes,)) as executor:
        
        futures = {executor.submit(load_and_process_one_file, path, is_val): path 
                   for path, is_val in tasks}
        
        pbar = tqdm(total=len(futures), desc="Loading H5ADs")
        
        for future in as_completed(futures):
            try:
                adata, status = future.result()
                
                if status == "Success":
                    all_adatas.append(adata)
                    stats["Success"] += 1
                elif status.startswith("Skipped"):
                    stats["Skipped"] += 1
                else:
                    stats["Errors"] += 1
                    
            except Exception as exc:
                tqdm.write(f"Critical exception: {exc}")
                stats["Errors"] += 1
            
            pbar.update(1)
        
        pbar.close()
    
    print(f"âœ… Loaded: {stats['Success']} | â­ï¸  Skipped: {stats['Skipped']} | âŒ Errors: {stats['Errors']}")
    return all_adatas

def write_shards(all_adatas, output_base, cells_per_shard=4000):
    """
    å°†æ‰€æœ‰ AnnData æŒ‰å›ºå®šç»†èƒæ•°åˆ‡åˆ†æˆå¤šä¸ª Shard å¹¶å†™å…¥ TileDB
    """
    if os.path.exists(output_base):
        print(f"ğŸ—‘ï¸  Removing existing output directory: {output_base}")
        shutil.rmtree(output_base)
    
    os.makedirs(output_base)
    print(f"ğŸ“ Created output directory: {output_base}")
    
    # 1. åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆæ³¨æ„ï¼šå¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯æ”¹ä¸ºæµå¼å¤„ç†ï¼‰
    print("ğŸ”— Concatenating all AnnData objects...")
    combined = ad.concat(all_adatas, join='outer', merge='same')
    
    total_cells = combined.n_obs
    n_shards = (total_cells + cells_per_shard - 1) // cells_per_shard
    
    print(f"ğŸ“Š Total cells: {total_cells:,}")
    print(f"ğŸ“¦ Target shard size: {cells_per_shard} cells/shard")
    print(f"ğŸ“¦ Will create {n_shards} shards")
    
    # 2. æ‰“ä¹±é¡ºåºï¼ˆç¡®ä¿æ¯ä¸ª Shard çš„æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼‰
    print("ğŸ”€ Shuffling cells for balanced shards...")
    perm = np.random.permutation(total_cells)
    combined = combined[perm, :].copy()
    
    # 3. åˆ‡åˆ†å¹¶å†™å…¥
    print(f"ğŸ’¾ Writing shards to {output_base}/...")
    
    for shard_idx in tqdm(range(n_shards), desc="Writing Shards"):
        start = shard_idx * cells_per_shard
        end = min(start + cells_per_shard, total_cells)
        
        shard_data = combined[start:end, :].copy()
        
        # Shard å‘½åï¼šshard_0000, shard_0001, ...
        shard_name = f"shard_{shard_idx:04d}"
        shard_uri = os.path.join(output_base, shard_name)
        
        tiledbsoma.io.from_anndata(
            experiment_uri=shard_uri,
            anndata=shard_data,
            measurement_name="RNA"
        )
    
    print(f"âœ… Successfully created {n_shards} shards with ~{cells_per_shard} cells each")
    return n_shards

if __name__ == "__main__":
    print("="*60)
    print("TileDB é¢„å¤„ç†è„šæœ¬ - å›ºå®š Shard å¤§å°ç‰ˆæœ¬")
    print("="*60)
    
    # 1. åŠ è½½åŸºå› é¡ºåº
    print("\nğŸ“– Loading gene order...")
    target_genes = pd.read_csv(GENE_ORDER_PATH, sep='\t', header=None)[0].values
    global_target_genes = target_genes
    global_target_gene_map = {gene: i for i, gene in enumerate(target_genes)}
    print(f"   Target genes: {len(target_genes)}")
    
    # 2. åŠ è½½æ–‡ä»¶åˆ—è¡¨
    print("\nğŸ“‹ Loading file list...")
    df = pd.read_csv(CSV_PATH)
    print(f"   Total files: {len(df)}")
    
    # 3. å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡ä»¶
    all_adatas = batch_load_files(df, max_workers=MAX_WORKERS)
    
    if len(all_adatas) == 0:
        print("âŒ No valid data loaded. Exiting.")
        exit(1)
    
    # 4. åˆ‡åˆ†å¹¶å†™å…¥ Shard
    n_shards = write_shards(all_adatas, OUTPUT_BASE_URI, cells_per_shard=CELLS_PER_SHARD)
    
    print("\n" + "="*60)
    print("âœ… Processing completed!")
    print(f"ğŸ“¦ Created {n_shards} shards")
    print(f"ğŸ’¾ Data saved to: {OUTPUT_BASE_URI}/")
    print("="*60)

