import os
import tiledbsoma
import math
import multiprocessing
from concurrent.futures import ProcessPoolExecutor
from typing import Tuple, List, Dict
import heapq

def _count_one_shard(args: Tuple[str, int]) -> Tuple[str, int]:
    """
    å•ä¸ª Worker çš„ä»»åŠ¡ï¼šè®¡ç®—ä¸€ä¸ªåˆ†ç‰‡é‡Œç¬¦åˆ split_label çš„ç»†èƒæ•°
    å¿…é¡»æ˜¯é¡¶å±‚å‡½æ•°ï¼Œä»¥ä¾¿ pickle åºåˆ—åŒ–ã€‚
    
    Returns:
        (shard_name, cell_count)
    """
    uri, split_label = args
    try:
        # æ˜¾å¼åˆ›å»ºç‹¬ç«‹çš„ Contextï¼Œé¿å…å¤šè¿›ç¨‹å…±äº« Context å¯¼è‡´çš„ C++ å±‚æ­»é”
        ctx = tiledbsoma.SOMATileDBContext()
        with tiledbsoma.Experiment.open(uri, context=ctx) as exp:
            query = exp.obs.read(
                value_filter=f"split_label == {split_label}",
                column_names=["soma_joinid"]
            ).concat()
            shard_name = os.path.basename(uri)
            return (shard_name, len(query))
    except Exception:
        return (os.path.basename(uri), 0)

def balanced_shard_assignment(
    shard_sizes: Dict[str, int], 
    num_workers: int
) -> Dict[int, List[str]]:
    """
    ä½¿ç”¨è´ªå¿ƒç®—æ³•å°† shards åˆ†é…ç»™ workersï¼Œä½¿æ¯ä¸ª worker çš„æ€»ç»†èƒæ•°å°½å¯èƒ½å‡è¡¡ã€‚
    
    ç®—æ³•ï¼šå¤šè·¯å½’å¹¶ï¼ˆç±»ä¼¼ Multiway Number Partitioningï¼‰
    1. ç»´æŠ¤ä¸€ä¸ªæœ€å°å †ï¼Œè®°å½•æ¯ä¸ª worker çš„å½“å‰æ€»ç»†èƒæ•°
    2. å¯¹ shards æŒ‰å¤§å°é™åºæ’åº
    3. æ¯æ¬¡å°†æœ€å¤§çš„ shard åˆ†é…ç»™å½“å‰è´Ÿè½½æœ€å°çš„ worker
    
    Args:
        shard_sizes: {shard_name: cell_count}
        num_workers: å…¨å±€ worker æ€»æ•° (world_size * num_workers_per_gpu)
        
    Returns:
        {worker_id: [shard_names]}
    """
    # åˆå§‹åŒ–ï¼šæ¯ä¸ª worker çš„è´Ÿè½½å’Œåˆ†é…åˆ—è¡¨
    # ä½¿ç”¨æœ€å°å †ï¼š(å½“å‰æ€»ç»†èƒæ•°, worker_id)
    heap = [(0, i) for i in range(num_workers)]
    heapq.heapify(heap)
    
    # æ¯ä¸ª worker åˆ†é…åˆ°çš„ shards
    assignment = {i: [] for i in range(num_workers)}
    
    # æŒ‰ shard å¤§å°é™åºæ’åºï¼ˆä¼˜å…ˆåˆ†é…å¤§ shardï¼‰
    sorted_shards = sorted(shard_sizes.items(), key=lambda x: x[1], reverse=True)
    
    # è´ªå¿ƒåˆ†é…
    for shard_name, cell_count in sorted_shards:
        if cell_count == 0:  # è·³è¿‡ç©º shard
            continue
        # å–å‡ºå½“å‰è´Ÿè½½æœ€å°çš„ worker
        current_load, worker_id = heapq.heappop(heap)
        # åˆ†é… shard
        assignment[worker_id].append(shard_name)
        # æ›´æ–°è´Ÿè½½å¹¶æ”¾å›å †
        heapq.heappush(heap, (current_load + cell_count, worker_id))
    
    # æ‰“å°è´Ÿè½½åˆ†å¸ƒï¼ˆç”¨äºè°ƒè¯•ï¼‰
    final_loads = sorted(heap, key=lambda x: x[0])
    min_load = final_loads[0][0]
    max_load = final_loads[-1][0]
    avg_load = sum(x[0] for x in final_loads) / len(final_loads)
    imbalance = (max_load - min_load) / avg_load * 100 if avg_load > 0 else 0
    
    print(f"âš–ï¸  [Load Balancing] Workers: {num_workers}")
    print(f"   Min: {min_load:,} cells | Max: {max_load:,} cells | Avg: {avg_load:,.0f} cells")
    print(f"   Imbalance: {imbalance:.2f}%")
    
    return assignment

def get_dataset_stats(
    root_dir: str, 
    split_label: int, 
    batch_size: int, 
    num_workers: int = 16, 
    world_size: int = 1,
    num_workers_per_gpu: int = 16
) -> Tuple[int, int, Dict[str, int]]:
    """
    å¤šè¿›ç¨‹å¹¶è¡Œæ‰«æ TileDB æ•°æ®é›†ï¼Œè®¡ç®—æ€»ç»†èƒæ•°ã€æ­¥æ•°å’Œæ¯ä¸ª shard çš„å¤§å°ã€‚
    
    Args:
        root_dir: æ•°æ®é›†æ ¹ç›®å½•
        split_label: 0=Train, 1=Val
        batch_size: å•å¡ Batch Size
        num_workers: å¹¶è¡Œæ‰«æçš„è¿›ç¨‹æ•° (å»ºè®®è®¾ä¸º CPU æ ¸å¿ƒæ•°çš„ä¸€åŠ)
        world_size: DDP æ€» GPU æ•°
        num_workers_per_gpu: æ¯ä¸ª GPU çš„ DataLoader workers æ•°
        
    Returns:
        (total_cells, total_steps, shard_sizes_dict)
    """
    if not os.path.exists(root_dir):
        print(f"âš ï¸ [Stats] è·¯å¾„ä¸å­˜åœ¨: {root_dir}")
        return 0, 0, {}
        
    sub_uris = sorted([
        os.path.join(root_dir, d) 
        for d in os.listdir(root_dir) 
        if os.path.isdir(os.path.join(root_dir, d))
    ])
    
    if not sub_uris:
        return 0, 0, {}
    
    print(f"ğŸ“Š [Stats] å¯åŠ¨å¤šè¿›ç¨‹æ‰«æ {len(sub_uris)} ä¸ª Shards (Split={split_label})...")
    
    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    tasks = [(uri, split_label) for uri in sub_uris]
    
    # åŠ¨æ€è°ƒæ•´ worker æ•°ï¼Œä¸è¶…è¿‡ä»»åŠ¡æ•°ä¹Ÿä¸è¶…è¿‡ CPU æ ¸å¿ƒæ•°
    max_workers = min(num_workers, len(tasks), os.cpu_count() or 1)

    # ä½¿ç”¨ ProcessPoolExecutor å¹¶è¡Œå¤„ç†
    shard_sizes = {}
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        results = executor.map(_count_one_shard, tasks)
        for shard_name, cell_count in results:
            shard_sizes[shard_name] = cell_count
    
    total_cells = sum(shard_sizes.values())
    
    # è®¡ç®— DDP ç¯å¢ƒä¸‹çš„ Global Batch Size
    global_batch_size = batch_size * world_size
    if global_batch_size == 0:
        return 0, 0, shard_sizes
        
    total_steps = math.ceil(total_cells / global_batch_size)
    
    print(f"âœ… [Stats] å®Œæˆ: {total_cells} cells | Global Batch: {global_batch_size} | Epoch Steps: {total_steps}")
    
    return total_cells, total_steps, shard_sizes
