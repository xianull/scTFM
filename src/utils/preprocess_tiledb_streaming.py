"""
æµå¼ç‰ˆ TileDB é¢„å¤„ç†è„šæœ¬ï¼šé€‚é…è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆ1äº¿+ ç»†èƒï¼‰
å†…å­˜å ç”¨ä¼˜åŒ–ï¼šä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼Œä½¿ç”¨å›ºå®šå¤§å°çš„ Buffer æµå¼å†™å…¥

å…³é”®è®¾è®¡ï¼š
1. Cell Bufferï¼šåªåœ¨å†…å­˜ä¸­ä¿æŒ BUFFER_SIZE ä¸ªç»†èƒï¼ˆå¦‚ 20 ä¸‡ï¼‰
2. æµå¼å†™å…¥ï¼šBuffer æ»¡äº†ç«‹å³å†™å…¥ Shardï¼Œé‡Šæ”¾å†…å­˜
3. å…¨å±€æ‰“ä¹±ï¼šé€šè¿‡æ–‡ä»¶çº§éšæœºè¯»å– + Shard å†…æ‰“ä¹±å®ç°æ•°æ®å‡åŒ€åˆ†å¸ƒ
4. å†…å­˜å³°å€¼ï¼š~10-20GBï¼ˆå–å†³äº BUFFER_SIZEï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- æ•°æ®é‡ï¼š1äº¿+ ç»†èƒ
- å¯ç”¨å†…å­˜ï¼š<2TB
- ç›®æ ‡ï¼šç”Ÿæˆå‡åŒ€å¤§å°çš„ Shards
"""

import multiprocessing
import os
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

import shutil
import pandas as pd
import numpy as np
import scanpy as sc
import anndata as ad
import scipy.sparse
import tiledbsoma
import tiledbsoma.io
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import gc

warnings.filterwarnings('ignore')

# ==================== é…ç½®å‚æ•° ====================
CSV_PATH = '/gpfs/flash/home/jcw/projects/research/cellTime/scTFM/data/ae_data_info.csv'
GENE_ORDER_PATH = "/gpfs/flash/home/jcw/projects/research/cellTime/scTFM/data/gene_order.tsv"
OUTPUT_BASE_URI = "/fast/data/scTFM/ae/tile_4000_stream"

CELLS_PER_SHARD = 8192      # æ¯ä¸ª Shard çš„ç›®æ ‡ç»†èƒæ•°
BUFFER_SIZE = 409600        # å†…å­˜ä¸­æœ€å¤šç¼“å­˜çš„ç»†èƒæ•°ï¼ˆ40ä¸‡ç»†èƒ â‰ˆ 10-20GB å†…å­˜ï¼‰
MAX_WORKERS = 16            # å¹¶è¡Œè¯»å–æ–‡ä»¶çš„è¿›ç¨‹æ•°
SHUFFLE_FILES = True        # æ˜¯å¦éšæœºæ‰“ä¹±æ–‡ä»¶è¯»å–é¡ºåºï¼ˆç¡®ä¿æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼‰

# ==================== å…¨å±€å˜é‡ ====================
global_target_genes = None
global_target_gene_map = None

def worker_init(gene_list):
    """å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½ç›®æ ‡åŸºå› åˆ—è¡¨"""
    global global_target_genes, global_target_gene_map
    global_target_genes = gene_list
    global_target_gene_map = {gene: i for i, gene in enumerate(gene_list)}

def load_and_process_one_file(file_path, is_full_val):
    """
    åŠ è½½å¹¶é¢„å¤„ç†å•ä¸ª h5ad æ–‡ä»¶
    è¿”å›å¤„ç†åçš„ AnnDataï¼ˆå·²å¯¹é½ã€å½’ä¸€åŒ–ã€log1pï¼‰
    """
    try:
        if not os.path.exists(file_path):
            return None, f"Missing"
        
        # 1. è¯»å–æ•°æ®
        adata = sc.read_h5ad(file_path)
        
        # 2. å‡†å¤‡å˜é‡å
        adata.var_names = adata.var['gene_symbols'].astype(str)
        adata.var_names_make_unique()
        
        # 3. è¿‡æ»¤ä½è´¨é‡ç»†èƒ
        sc.pp.filter_cells(adata, min_genes=200)
        
        if adata.n_obs == 0:
            return None, "Skipped"
        
        # 4. åŸºå› å¯¹é½
        target_genes = global_target_genes
        target_n_vars = len(target_genes)
        target_gene_map = global_target_gene_map
        
        common_genes = [g for g in adata.var_names if g in target_gene_map]
        
        if len(common_genes) == 0:
            new_X = scipy.sparse.csr_matrix((adata.n_obs, target_n_vars), dtype=np.float32)
            adata = ad.AnnData(X=new_X, obs=adata.obs)
            adata.var_names = target_genes
        else:
            adata = adata[:, common_genes].copy()
            
            if not scipy.sparse.isspmatrix_csr(adata.X):
                adata.X = adata.X.tocsr()
            
            current_col_to_target_col = np.array(
                [target_gene_map[g] for g in adata.var_names], 
                dtype=np.int32
            )
            new_indices = current_col_to_target_col[adata.X.indices]
            
            new_X = scipy.sparse.csr_matrix(
                (adata.X.data, new_indices, adata.X.indptr),
                shape=(adata.n_obs, target_n_vars)
            )
            new_X.sort_indices()
            
            adata = ad.AnnData(X=new_X, obs=adata.obs)
            adata.var_names = target_genes
        
        # 5. å½’ä¸€åŒ–å’Œ log1p
        sc.pp.normalize_total(adata, target_sum=1e4)
        sc.pp.log1p(adata)
        
        # 6. æ‰“æ ‡ç­¾
        if is_full_val == 1:
            adata.obs['split_label'] = 3  # Test OOD
        else:
            n_cells = adata.n_obs
            split_labels = np.random.choice(
                [0, 1, 2], 
                size=n_cells, 
                p=[0.9, 0.05, 0.05]
            )
            adata.obs['split_label'] = split_labels
        
        adata.obs['split_label'] = adata.obs['split_label'].astype(np.int32)
        
        # 7. ç¡®ä¿ float32
        if adata.X.dtype != np.float32:
            adata.X = adata.X.astype(np.float32)
        
        return adata, "Success"
    
    except Exception as e:
        return None, f"Error: {str(e)[:50]}"


class StreamingShardWriter:
    """
    æµå¼ Shard å†™å…¥å™¨
    ç»´æŠ¤ä¸€ä¸ªå›ºå®šå¤§å°çš„ Cell Bufferï¼Œæ»¡äº†å°±å†™å…¥ç£ç›˜
    """
    def __init__(self, output_base, cells_per_shard=4000, buffer_size=200000):
        self.output_base = output_base
        self.cells_per_shard = cells_per_shard
        self.buffer_size = buffer_size
        
        # æ¸…ç©ºè¾“å‡ºç›®å½•
        if os.path.exists(output_base):
            print(f"ğŸ—‘ï¸  Removing existing output: {output_base}")
            shutil.rmtree(output_base)
        os.makedirs(output_base)
        print(f"ğŸ“ Created output directory: {output_base}")
        
        # çŠ¶æ€
        self.buffer = []           # å½“å‰ Buffer ä¸­çš„ AnnData åˆ—è¡¨
        self.buffer_cells = 0      # Buffer ä¸­çš„ç»†èƒæ€»æ•°
        self.shard_idx = 0         # å½“å‰ Shard ç´¢å¼•
        self.total_cells = 0       # å·²å¤„ç†çš„æ€»ç»†èƒæ•°
        
    def add_adata(self, adata):
        """
        æ·»åŠ ä¸€ä¸ª AnnData åˆ° Buffer
        å¦‚æœ Buffer æ»¡äº†ï¼Œè‡ªåŠ¨ flush
        """
        self.buffer.append(adata)
        self.buffer_cells += adata.n_obs
        self.total_cells += adata.n_obs
        
        # Buffer æ»¡äº†ï¼Œå†™å…¥ç£ç›˜
        if self.buffer_cells >= self.buffer_size:
            self._flush_buffer()
    
    def _flush_buffer(self):
        """
        å°† Buffer ä¸­çš„æ‰€æœ‰ç»†èƒåˆå¹¶ã€æ‰“ä¹±ã€åˆ‡åˆ†æˆå¤šä¸ª Shards å¹¶å†™å…¥
        """
        if len(self.buffer) == 0:
            return
        
        n_shards_in_buffer = (self.buffer_cells + self.cells_per_shard - 1) // self.cells_per_shard
        print(f"\nğŸ’¾ Flushing buffer: {self.buffer_cells:,} cells â†’ {n_shards_in_buffer} shards...")
        
        # 1. åˆå¹¶ Buffer
        print(f"   1/4 Concatenating {len(self.buffer)} AnnData objects...")
        combined = ad.concat(self.buffer, join='outer', merge='same')
        n_cells = combined.n_obs
        print(f"       âœ“ Merged: {n_cells:,} cells")
        
        # 2. æ‰“ä¹±ï¼ˆç¡®ä¿ Shard å†…æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼‰
        print(f"   2/4 Shuffling cells...")
        perm = np.random.permutation(n_cells)
        combined = combined[perm, :].copy()
        print(f"       âœ“ Shuffled")
        
        # 3. åˆ‡åˆ†æˆå¤šä¸ª Shards
        print(f"   3/4 Splitting into {n_shards_in_buffer} shards...")
        
        # 4. å†™å…¥ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        print(f"   4/4 Writing to TileDB...")
        for i in tqdm(range(n_shards_in_buffer), desc="       Writing", leave=False):
            start = i * self.cells_per_shard
            end = min(start + self.cells_per_shard, n_cells)
            
            shard_data = combined[start:end, :].copy()
            
            shard_name = f"shard_{self.shard_idx:05d}"
            shard_uri = os.path.join(self.output_base, shard_name)
            
            tiledbsoma.io.from_anndata(
                experiment_uri=shard_uri,
                anndata=shard_data,
                measurement_name="RNA"
            )
            
            self.shard_idx += 1
        
        print(f"       âœ“ Wrote shards {self.shard_idx - n_shards_in_buffer} to {self.shard_idx - 1}")
        
        # 5. æ¸…ç©º Bufferï¼Œé‡Šæ”¾å†…å­˜
        self.buffer.clear()
        self.buffer_cells = 0
        del combined
        gc.collect()
    
    def finalize(self):
        """
        å¤„ç†å®Œæ‰€æœ‰æ–‡ä»¶åï¼Œå†™å…¥å‰©ä½™çš„ Buffer
        """
        if len(self.buffer) > 0:
            self._flush_buffer()
        
        print(f"\nâœ… Streaming write completed:")
        print(f"   Total shards: {self.shard_idx}")
        print(f"   Total cells: {self.total_cells:,}")
        print(f"   Avg cells/shard: {self.total_cells / max(1, self.shard_idx):.1f}")


def streaming_process(df, writer, max_workers=16, shuffle=True):
    """
    æµå¼å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œè¾¹è¯»è¾¹å†™
    """
    print(f"\nğŸš€ Starting streaming processing...")
    print(f"   Files: {len(df)}")
    print(f"   Workers: {max_workers}")
    print(f"   Buffer size: {writer.buffer_size:,} cells")
    print(f"   Shard size: {writer.cells_per_shard} cells")
    
    # éšæœºæ‰“ä¹±æ–‡ä»¶é¡ºåºï¼ˆç¡®ä¿æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼‰
    if shuffle:
        df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)
        print(f"   ğŸ”€ File order shuffled")
    
    tasks = [(row['file_path'], row['full_validation_dataset']) 
             for _, row in df.iterrows()]
    
    stats = {"Success": 0, "Skipped": 0, "Errors": 0}
    
    with ProcessPoolExecutor(max_workers=max_workers, 
                             initializer=worker_init, 
                             initargs=(global_target_genes,)) as executor:
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(load_and_process_one_file, path, is_val): (idx, path) 
                   for idx, (path, is_val) in enumerate(tasks)}
        
        pbar = tqdm(total=len(futures), desc="Processing & Writing")
        
        for future in as_completed(futures):
            idx, file_path = futures[future]
            
            try:
                adata, status = future.result()
                
                if status == "Success":
                    # ç«‹å³å†™å…¥ Writerï¼ˆå¯èƒ½è§¦å‘ flushï¼‰
                    writer.add_adata(adata)
                    stats["Success"] += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    pbar.set_postfix({
                        "Cells": f"{writer.total_cells:,}",
                        "Shards": writer.shard_idx,
                        "Buffer": f"{writer.buffer_cells:,}"
                    })
                    
                elif status.startswith("Skipped"):
                    stats["Skipped"] += 1
                else:
                    stats["Errors"] += 1
                    
            except Exception as exc:
                tqdm.write(f"âŒ Critical exception: {exc}")
                stats["Errors"] += 1
            
            pbar.update(1)
        
        pbar.close()
    
    print(f"\nğŸ“Š Processing stats:")
    print(f"   âœ… Success: {stats['Success']}")
    print(f"   â­ï¸  Skipped: {stats['Skipped']}")
    print(f"   âŒ Errors: {stats['Errors']}")


if __name__ == "__main__":
    print("="*70)
    print("TileDB é¢„å¤„ç†è„šæœ¬ - æµå¼å¤„ç†ç‰ˆæœ¬ï¼ˆé€‚é…è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼‰")
    print("="*70)
    
    # 1. åŠ è½½åŸºå› é¡ºåº
    print("\nğŸ“– Loading gene order...")
    target_genes = pd.read_csv(GENE_ORDER_PATH, sep='\t', header=None)[0].values
    global_target_genes = target_genes
    global_target_gene_map = {gene: i for i, gene in enumerate(target_genes)}
    print(f"   Target genes: {len(target_genes):,}")
    
    # 2. åŠ è½½æ–‡ä»¶åˆ—è¡¨
    print("\nğŸ“‹ Loading file list...")
    df = pd.read_csv(CSV_PATH)
    print(f"   Total files: {len(df):,}")
    
    # 3. åˆ›å»ºæµå¼å†™å…¥å™¨
    writer = StreamingShardWriter(
        output_base=OUTPUT_BASE_URI,
        cells_per_shard=CELLS_PER_SHARD,
        buffer_size=BUFFER_SIZE
    )
    
    # 4. æµå¼å¤„ç†
    streaming_process(
        df=df,
        writer=writer,
        max_workers=MAX_WORKERS,
        shuffle=SHUFFLE_FILES
    )
    
    # 5. å®Œæˆå†™å…¥
    writer.finalize()
    
    print("\n" + "="*70)
    print("âœ… All done!")
    print(f"ğŸ’¾ Data saved to: {OUTPUT_BASE_URI}/")
    print("="*70)

