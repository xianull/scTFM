# VAE é…ç½®æŒ‡å—

## ğŸ“ é…ç½®æ–‡ä»¶æ¦‚è§ˆ

### åŸºç¡€é…ç½®
- **`vae.yaml`** - æ ‡å‡† VAE é…ç½®
- **`vae_beta.yaml`** - Î²-VAE (å¢å¼ºè§£è€¦æ€§)
- **`vae_lightweight.yaml`** - è½»é‡çº§ VAE (å¿«é€Ÿå®éªŒ)

### å®Œæ•´æ¨¡å‹é…ç½®ï¼ˆåŒ…å«è®­ç»ƒå‚æ•°ï¼‰
- **`ae_stage1_vae.yaml`** - æ ‡å‡† VAE å®Œæ•´é…ç½®
- **`ae_stage1_beta_vae.yaml`** - Î²-VAE å®Œæ•´é…ç½®

---

## ğŸ”§ å…³é”®è¶…å‚æ•°

### 1. **KL æ•£åº¦æƒé‡ (`kld_weight`)** - æœ€é‡è¦ï¼

æ§åˆ¶ KL æ•£åº¦åœ¨æ€»æŸå¤±ä¸­çš„æ¯”é‡ï¼š
```yaml
model:
  kld_weight: 0.00001  # é…ç½®åœ¨ model config ä¸­
```

**æ¨èå€¼**ï¼š
- **æ ‡å‡† VAE**: `1e-5` ~ `1e-4`
  - å¹³è¡¡é‡æ„è´¨é‡å’Œæ½œåœ¨ç©ºé—´è§„æ•´æ€§
  - é€‚åˆå¤§å¤šæ•°åœºæ™¯
  
- **Î²-VAE (è§£è€¦è¡¨ç¤º)**: `1e-3` ~ `1e-2`
  - æ›´å¼ºçš„ KL çº¦æŸï¼Œç‰ºç‰²é‡æ„æ¢å–è§£è€¦æ€§
  - é€‚åˆéœ€è¦å¯è§£é‡Šæ½œåœ¨å› å­çš„åœºæ™¯
  
- **è½»é‡ KL (é˜²æ­¢åéªŒåå¡Œ)**: `1e-6` ~ `1e-5`
  - å‡ ä¹ä¸çº¦æŸ KLï¼Œé‡æ„ä¼˜å…ˆ
  - é€‚åˆåˆæœŸè®­ç»ƒæˆ–é«˜ç»´æ•°æ®

**è°ƒå‚ç­–ç•¥**ï¼š
```python
# ä»å°åˆ°å¤§é€æ­¥å¢åŠ 
kld_weight: [1e-6, 1e-5, 1e-4, 1e-3]

# ç›‘æ§æŒ‡æ ‡
train/kl_loss      # åº”è¯¥åœ¨ 10-100 ä¹‹é—´ï¼ˆè¿‡ä½è¯´æ˜åéªŒåå¡Œï¼‰
train/recon_loss   # é‡æ„æŸå¤±ï¼ˆä¸åº”æ˜¾è‘—å˜å·®ï¼‰
val/loss           # éªŒè¯æŸå¤±ï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰
```

---

### 2. **æ½œåœ¨ç©ºé—´ç»´åº¦ (`latent_dim`)**

```yaml
model:
  net:
    latent_dim: 64  # é…ç½®åœ¨ net config ä¸­
```

**æ¨èå€¼**ï¼š
- **å°è§„æ¨¡æ•°æ®**: 32 ~ 64
- **ä¸­ç­‰è§„æ¨¡**: 64 ~ 128
- **å¤§è§„æ¨¡/å¤æ‚æ•°æ®**: 128 ~ 512

**æƒè¡¡**ï¼š
- âœ… æ›´å¤§ï¼šè¡¨è¾¾èƒ½åŠ›å¼ºï¼Œä½†è®¡ç®—é‡å¤§
- âœ… æ›´å°ï¼šå¿«é€Ÿè®­ç»ƒï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆ

---

### 3. **ç½‘ç»œæ¶æ„ (`hidden_dims`)**

```yaml
model:
  net:
    hidden_dims: [2048, 1024, 512]  # Encoder å±‚
    # Decoder ä¼šè‡ªåŠ¨é•œåƒä¸º [512, 1024, 2048]
```

**æ¨èé…ç½®**ï¼š
```yaml
# è½»é‡çº§ï¼ˆå¿«é€Ÿå®éªŒï¼‰
hidden_dims: [1024, 512]

# æ ‡å‡†ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰
hidden_dims: [2048, 1024, 512]

# æ·±å±‚ï¼ˆé«˜è¡¨è¾¾èƒ½åŠ›ï¼‰
hidden_dims: [4096, 2048, 1024, 512]
```

---

### 4. **æ¿€æ´»å‡½æ•° (`activation`)**

```yaml
model:
  net:
    activation: "GELU"  # LeakyReLU, GELU, SiLU, ReLU, SwiGLU
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- **GELU**: å¹³æ»‘ï¼Œè®­ç»ƒç¨³å®šï¼Œæ¨èé¦–é€‰ âœ…
- **SiLU**: ç±»ä¼¼ GELUï¼Œæ€§èƒ½ç•¥å¥½
- **LeakyReLU**: è®¡ç®—å¿«ï¼Œä¼ ç»Ÿé€‰æ‹©
- **SwiGLU**: æœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œä½†éœ€è¦æ›´å¤šå‚æ•° âš ï¸

**æ³¨æ„**ï¼šSwiGLU ä¼šè‡ªåŠ¨ä½¿ç”¨ LayerNorm è€Œé BatchNormï¼

---

### 5. **æ­£åˆ™åŒ– (`dropout_rate`, `use_batch_norm`)**

```yaml
model:
  net:
    dropout_rate: 0.1     # Dropout æ¯”ä¾‹
    use_batch_norm: True  # æ˜¯å¦ä½¿ç”¨ Normalization
```

**æ¨èå€¼**ï¼š
- **Dropout**: 0.05 ~ 0.15ï¼ˆè¿‡é«˜ä¼šæŸå®³æ€§èƒ½ï¼‰
- **BatchNorm**: é€šå¸¸å»ºè®®å¯ç”¨ï¼ˆSwiGLU é™¤å¤–ï¼‰

---

## ğŸ“Š å®Œæ•´é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ ‡å‡† VAEï¼ˆæ¨èèµ·ç‚¹ï¼‰

```yaml
# configs/model/net/ae/vae.yaml
_target_: src.models.components.ae.vae.VariationalAE
input_dim: 28231
hidden_dims: [2048, 1024, 512]
latent_dim: 64
dropout_rate: 0.1
use_batch_norm: True
activation: "GELU"
```

```yaml
# configs/model/ae_stage1_vae.yaml
model:
  _target_: src.models.ae_module.AELitModule
  kld_weight: 0.00001  # å…³é”®ï¼šKL æƒé‡
  
  optimizer:
    lr: 0.0001
    weight_decay: 1e-5
```

**è¿è¡Œ**ï¼š
```bash
python train.py experiment=stage1_ae model=ae_stage1_vae
```

---

### ç¤ºä¾‹2ï¼šÎ²-VAEï¼ˆå¢å¼ºè§£è€¦æ€§ï¼‰

```yaml
# configs/model/net/ae/vae_beta.yaml
hidden_dims: [4096, 2048, 1024, 512]  # æ›´æ·±
latent_dim: 128                        # æ›´å¤§
dropout_rate: 0.15
```

```yaml
# configs/model/ae_stage1_beta_vae.yaml
model:
  kld_weight: 0.001  # 10-100x æ ‡å‡† VAE
```

**è¿è¡Œ**ï¼š
```bash
python train.py experiment=stage1_ae model=ae_stage1_beta_vae
```

---

### ç¤ºä¾‹3ï¼šè½»é‡çº§ VAEï¼ˆå¿«é€Ÿå®éªŒï¼‰

```yaml
# configs/model/net/ae/vae_lightweight.yaml
hidden_dims: [1024, 512]  # æ›´æµ…
latent_dim: 32             # æ›´å°
dropout_rate: 0.05
activation: "LeakyReLU"    # æ›´å¿«
```

---

## ğŸ”¬ è¶…å‚æ•° Sweep

### Sweep KL æƒé‡ï¼ˆæœ€é‡è¦ï¼‰

```yaml
# configs/experiment/stage1_vae_sweep.yaml
hydra:
  sweeper:
    params:
      model.kld_weight: 1e-6,1e-5,1e-4,1e-3
      model.net.latent_dim: 32,64,128,256
```

**è¿è¡Œ**ï¼š
```bash
python train.py experiment=stage1_vae_sweep
```

**æ€»è¿è¡Œæ•°**ï¼š4 (KL) Ã— 4 (latent_dim) = 16 æ¬¡

---

### å¯¹æ¯”ä¸åŒå˜ä½“

```bash
# å‘½ä»¤è¡Œ Sweep
python train.py experiment=stage1_ae \
  model=ae_stage1_vae,ae_stage1_beta_vae \
  model.net.latent_dim=64,128 \
  -m
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### è®­ç»ƒæ—¶å…³æ³¨ï¼š

1. **`train/recon_loss`** - é‡æ„æŸå¤±
   - åº”è¯¥å¹³æ»‘ä¸‹é™
   - æœ€ç»ˆå€¼ï¼š0.01 ~ 0.1ï¼ˆå–å†³äºæ•°æ®ï¼‰

2. **`train/kl_loss`** - KL æ•£åº¦
   - åº”è¯¥åœ¨ 10 ~ 100 ä¹‹é—´
   - âš ï¸ **è¿‡ä½ï¼ˆ< 1ï¼‰**ï¼šåéªŒåå¡Œï¼Œéœ€è¦å¢åŠ  `kld_weight`
   - âš ï¸ **è¿‡é«˜ï¼ˆ> 500ï¼‰**ï¼šè¿‡åº¦çº¦æŸï¼Œé™ä½ `kld_weight`

3. **`train/loss`** - æ€»æŸå¤±
   - = recon_loss + kld_weight * kl_loss

4. **`val/loss`** - éªŒè¯æŸå¤±
   - æœ€ç»ˆä¼˜åŒ–ç›®æ ‡

### W&B å¯è§†åŒ–ï¼š

```python
# åœ¨ W&B ä¸­åˆ›å»ºè‡ªå®šä¹‰å›¾è¡¨
x: train/kl_loss
y: train/recon_loss
color: model.kld_weight
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### 1. **åéªŒåå¡Œ (Posterior Collapse)**

**ç—‡çŠ¶**ï¼š
- `train/kl_loss` æ¥è¿‘ 0
- VAE é€€åŒ–ä¸ºæ™®é€š AE

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# æ–¹æ³•1ï¼šå¢åŠ  KL æƒé‡
model.kld_weight: 0.0001  # ä» 1e-5 å¢åŠ åˆ° 1e-4

# æ–¹æ³•2ï¼šKL é€€ç« (éœ€è¦ä¿®æ”¹ä»£ç )
# ä» 0 é€æ­¥å¢åŠ åˆ°ç›®æ ‡å€¼

# æ–¹æ³•3ï¼šå¢å¤§æ½œåœ¨ç©ºé—´
model.net.latent_dim: 128  # ä» 64 å¢åŠ åˆ° 128
```

---

### 2. **é‡æ„è´¨é‡å·®**

**ç—‡çŠ¶**ï¼š
- `train/recon_loss` å¾ˆé«˜
- é‡æ„å›¾åƒæ¨¡ç³Š

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# æ–¹æ³•1ï¼šé™ä½ KL æƒé‡
model.kld_weight: 1e-6  # ä» 1e-4 é™ä½åˆ° 1e-6

# æ–¹æ³•2ï¼šå¢åŠ ç½‘ç»œå®¹é‡
model.net.hidden_dims: [4096, 2048, 1024, 512]

# æ–¹æ³•3ï¼šé™ä½ dropout
model.net.dropout_rate: 0.05
```

---

### 3. **è®­ç»ƒä¸ç¨³å®š / NaN**

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# å·²ä¿®å¤ SwiGLUï¼ä½†å¦‚æœè¿˜æœ‰é—®é¢˜ï¼š

# æ–¹æ³•1ï¼šé™ä½å­¦ä¹ ç‡
model.optimizer.lr: 5e-5

# æ–¹æ³•2ï¼šå¢å¤§æ¢¯åº¦è£å‰ª
trainer.gradient_clip_val: 1.0

# æ–¹æ³•3ï¼šåˆ‡æ¢æ¿€æ´»å‡½æ•°
model.net.activation: "GELU"  # ä¸è¦ç”¨ SwiGLU
```

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **ä»æ ‡å‡†é…ç½®å¼€å§‹**
   ```bash
   python train.py experiment=stage1_ae model=ae_stage1_vae
   ```

2. **Sweep KL æƒé‡**
   ```bash
   python train.py experiment=stage1_vae_sweep
   ```

3. **é€‰æ‹©æœ€ä½³ `kld_weight`**
   - åœ¨ W&B ä¸­å¯¹æ¯” `val/loss`
   - æ£€æŸ¥ `train/kl_loss` æ˜¯å¦åœ¨åˆç†èŒƒå›´ï¼ˆ10-100ï¼‰

4. **Fine-tune å…¶ä»–å‚æ•°**
   - è°ƒæ•´ `latent_dim`
   - è°ƒæ•´ç½‘ç»œæ·±åº¦/å®½åº¦
   - è°ƒæ•´å­¦ä¹ ç‡

5. **Benchmark è¯„ä¼°**
   ```bash
   python bench/benchmark_ae.py --dir logs/vae_kl_sweep/multiruns/
   ```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- **æ ‡å‡† VAE**: Kingma & Welling (2014) - Auto-Encoding Variational Bayes
- **Î²-VAE**: Higgins et al. (2017) - Î²-VAE: Learning Basic Visual Concepts
- **SwiGLU**: Shazeer (2020) - GLU Variants Improve Transformer

---

## ğŸ”— ç›¸å…³é…ç½®

- **Vanilla AE**: `configs/model/net/ae/vanilla.yaml`
- **RAE (L2æ­£åˆ™)**: `configs/model/net/ae/rae.yaml`
- **SAE (L1ç¨€ç–)**: `configs/model/net/ae/sae.yaml`

