#!/usr/bin/env python
"""
ç»†èƒè½¨è¿¹é‡‡æ ·è„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½è®­ç»ƒå¥½çš„ RTF æ¨¡å‹
2. ä»æŒ‡å®šèµ·å§‹ç»†èƒç”Ÿæˆæœªæ¥è½¨è¿¹
3. å¯è§†åŒ–ç”Ÿæˆçš„è½¨è¿¹ï¼ˆUMAP/t-SNEï¼‰
4. ä¿å­˜ç”Ÿæˆçš„ç»†èƒçŠ¶æ€

ä½¿ç”¨æ–¹æ³•ï¼š
# ä»æŒ‡å®šç»†èƒç”Ÿæˆè½¨è¿¹
python scripts/sample_trajectory.py \
    --ckpt_path logs/rtf/checkpoints/best.ckpt \
    --data_dir /fast/data/scTFM/rtf/TEDD/latents \
    --start_cell_id "SRX21870170_AAACCCAAGGAGAGTA-1" \
    --time_points 0.0,0.5,1.0,1.5,2.0 \
    --output_dir outputs/trajectories

# æ‰¹é‡ç”Ÿæˆå¤šæ¡è½¨è¿¹
python scripts/sample_trajectory.py \
    --ckpt_path logs/rtf/checkpoints/best.ckpt \
    --data_dir /fast/data/scTFM/rtf/TEDD/latents \
    --n_samples 100 \
    --time_points 0.0,0.5,1.0,1.5,2.0 \
    --output_dir outputs/trajectories
"""

import os
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

import torch
import tiledbsoma
import scanpy as sc
import anndata as ad

from src.models.flow_module import FlowLitModule


def load_cell_from_tiledb(data_dir: str, cell_id: str, measurement_name: str = "RNA"):
    """
    ä» TileDB-SOMA åŠ è½½æŒ‡å®šç»†èƒçš„ latent è¡¨ç¤ºã€‚
    
    Args:
        data_dir: TileDB æ ¹ç›®å½•
        cell_id: ç»†èƒ ID
        measurement_name: Measurement åç§°
    
    Returns:
        x: ç»†èƒçš„ latent å‘é‡ (latent_dim,)
        obs_data: ç»†èƒçš„å…ƒæ•°æ®ï¼ˆdictï¼‰
    """
    # æ‰«ææ‰€æœ‰ shards
    shard_dirs = [
        os.path.join(data_dir, d)
        for d in os.listdir(data_dir)
        if os.path.isdir(os.path.join(data_dir, d))
    ]
    
    ctx = tiledbsoma.SOMATileDBContext()
    
    for shard_uri in shard_dirs:
        try:
            with tiledbsoma.Experiment.open(shard_uri, context=ctx) as exp:
                obs = exp.obs.read().concat().to_pandas()
                
                if cell_id in obs.index:
                    # æ‰¾åˆ°äº†ï¼
                    X = exp.ms[measurement_name].X[measurement_name].read(
                        coords=([cell_id], slice(None))
                    ).tables().concat().to_pandas().to_numpy()
                    
                    obs_data = obs.loc[cell_id].to_dict()
                    
                    return X[0], obs_data
        except:
            continue
    
    raise ValueError(f"âŒ Cell ID '{cell_id}' not found in {data_dir}")


def sample_random_cells(data_dir: str, n_samples: int, split_label: int = 0, measurement_name: str = "RNA"):
    """
    ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·èµ·å§‹ç»†èƒã€‚
    
    Args:
        data_dir: TileDB æ ¹ç›®å½•
        n_samples: é‡‡æ ·æ•°é‡
        split_label: æ•°æ®é›†åˆ’åˆ† (0=Train, 1=Val)
        measurement_name: Measurement åç§°
    
    Returns:
        cells: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (cell_id, x, obs_data)
    """
    ctx = tiledbsoma.SOMATileDBContext()
    
    # æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç»†èƒ
    all_cells = []
    
    shard_dirs = [
        os.path.join(data_dir, d)
        for d in os.listdir(data_dir)
        if os.path.isdir(os.path.join(data_dir, d))
    ]
    
    print(f"ğŸ” æ‰«æ {len(shard_dirs)} ä¸ª shards...")
    
    for shard_uri in tqdm(shard_dirs, desc="æ”¶é›†ç»†èƒ"):
        try:
            with tiledbsoma.Experiment.open(shard_uri, context=ctx) as exp:
                obs = exp.obs.read().concat().to_pandas()
                
                # è¿‡æ»¤ split_label
                obs_filtered = obs[obs['split_label'] == split_label]
                
                if len(obs_filtered) == 0:
                    continue
                
                # è¯»å– X æ•°æ®
                X = exp.ms[measurement_name].X[measurement_name].read().tables().concat().to_pandas().to_numpy()
                
                # æ”¶é›†ç»†èƒ
                for cell_id in obs_filtered.index:
                    idx = list(obs.index).index(cell_id)
                    all_cells.append((cell_id, X[idx], obs_filtered.loc[cell_id].to_dict()))
        except:
            continue
    
    # éšæœºé‡‡æ ·
    if len(all_cells) < n_samples:
        print(f"âš ï¸  åªæ‰¾åˆ° {len(all_cells)} ä¸ªç»†èƒï¼Œå°äºè¯·æ±‚çš„ {n_samples}")
        n_samples = len(all_cells)
    
    indices = np.random.choice(len(all_cells), n_samples, replace=False)
    sampled_cells = [all_cells[i] for i in indices]
    
    return sampled_cells


def generate_trajectory(model, x_start, time_points, obs_data, device='cuda'):
    """
    ä»èµ·å§‹ç»†èƒç”Ÿæˆå®Œæ•´çš„æ—¶é—´è½¨è¿¹ã€‚
    
    Args:
        model: è®­ç»ƒå¥½çš„ Flow æ¨¡å‹
        x_start: èµ·å§‹ç»†èƒ (latent_dim,)
        time_points: æ—¶é—´ç‚¹åˆ—è¡¨ [0.0, 0.5, 1.0, 1.5, 2.0]
        obs_data: èµ·å§‹ç»†èƒçš„å…ƒæ•°æ®
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        trajectory: numpy array (n_time_points, latent_dim)
        metadata: æ¯ä¸ªæ—¶é—´ç‚¹çš„å…ƒæ•°æ®åˆ—è¡¨
    """
    trajectory = []
    metadata = []
    
    x_curr = torch.from_numpy(x_start).float().unsqueeze(0).to(device)
    trajectory.append(x_start)
    metadata.append({'time': time_points[0], **obs_data})
    
    for i in range(len(time_points) - 1):
        t_curr = time_points[i]
        t_next = time_points[i + 1]
        delta_t = t_next - t_curr
        
        cond_data = {
            'x_curr': x_curr,
            'time_curr': torch.tensor([t_curr]).to(device),
            'time_next': torch.tensor([t_next]).to(device),
            'delta_t': torch.tensor([delta_t]).to(device),
        }
        
        with torch.no_grad():
            x_next = model.flow.sample(x_curr, cond_data, steps=50, method='rk4')
        
        trajectory.append(x_next.cpu().numpy()[0])
        metadata.append({'time': t_next, **obs_data})
        
        x_curr = x_next
    
    return np.array(trajectory), metadata


def visualize_trajectories(trajectories_list, metadata_list, output_path):
    """
    å¯è§†åŒ–ç”Ÿæˆçš„è½¨è¿¹ã€‚
    
    Args:
        trajectories_list: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (n_time_points, latent_dim)
        metadata_list: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ metadata
        output_path: è¾“å‡ºè·¯å¾„
    """
    # åˆå¹¶æ‰€æœ‰è½¨è¿¹
    all_traj = np.concatenate(trajectories_list, axis=0)
    
    # åˆ›å»º AnnData
    adata = ad.AnnData(X=all_traj)
    
    # æ·»åŠ å…ƒæ•°æ®
    obs_data = []
    for traj_idx, metadata in enumerate(metadata_list):
        for time_idx, meta in enumerate(metadata):
            obs_data.append({
                'trajectory_id': traj_idx,
                'time': meta['time'],
                'time_step': time_idx,
            })
    
    adata.obs = pd.DataFrame(obs_data)
    
    # é™ç»´
    print("ğŸ¨ è®¡ç®— PCA...")
    sc.tl.pca(adata, n_comps=30)
    
    print("ğŸ¨ è®¡ç®— UMAP...")
    sc.pp.neighbors(adata, n_neighbors=15)
    sc.tl.umap(adata)
    
    # ç»˜å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # 1. æŒ‰è½¨è¿¹ ID ç€è‰²
    sc.pl.umap(adata, color='trajectory_id', ax=axes[0], show=False, title="Trajectory ID")
    
    # 2. æŒ‰æ—¶é—´ç€è‰²
    sc.pl.umap(adata, color='time', ax=axes[1], show=False, title="Time", cmap='viridis')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="ç»†èƒè½¨è¿¹é‡‡æ ·")
    parser.add_argument("--ckpt_path", type=str, required=True, help="RTF æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--data_dir", type=str, required=True, help="Latent TileDB æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="outputs/trajectories", help="è¾“å‡ºç›®å½•")
    
    # é‡‡æ ·æ¨¡å¼
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--start_cell_id", type=str, help="èµ·å§‹ç»†èƒ ID")
    group.add_argument("--n_samples", type=int, help="éšæœºé‡‡æ ·ç»†èƒæ•°é‡")
    
    # æ—¶é—´ç‚¹
    parser.add_argument("--time_points", type=str, required=True, help="æ—¶é—´ç‚¹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä¾‹å¦‚ '0.0,0.5,1.0,1.5,2.0'")
    parser.add_argument("--device", type=str, default="cuda", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--measurement_name", type=str, default="RNA", help="Measurement åç§°")
    
    args = parser.parse_args()
    
    # è§£ææ—¶é—´ç‚¹
    time_points = [float(t) for t in args.time_points.split(',')]
    print(f"â° æ—¶é—´ç‚¹: {time_points}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ”§ åŠ è½½ RTF æ¨¡å‹: {args.ckpt_path}")
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    model = FlowLitModule.load_from_checkpoint(args.ckpt_path, map_location=device)
    model.eval()
    model.to(device)
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    # 2. åŠ è½½èµ·å§‹ç»†èƒ
    if args.start_cell_id:
        print(f"ğŸ“¦ åŠ è½½èµ·å§‹ç»†èƒ: {args.start_cell_id}")
        x_start, obs_data = load_cell_from_tiledb(args.data_dir, args.start_cell_id, args.measurement_name)
        start_cells = [(args.start_cell_id, x_start, obs_data)]
    else:
        print(f"ğŸ² éšæœºé‡‡æ · {args.n_samples} ä¸ªèµ·å§‹ç»†èƒ...")
        start_cells = sample_random_cells(args.data_dir, args.n_samples, split_label=0, measurement_name=args.measurement_name)
        print(f"âœ… é‡‡æ ·å®Œæˆï¼å…± {len(start_cells)} ä¸ªç»†èƒ")
    
    # 3. ç”Ÿæˆè½¨è¿¹
    trajectories_list = []
    metadata_list = []
    
    print(f"ğŸš€ ç”Ÿæˆè½¨è¿¹...")
    for cell_id, x_start, obs_data in tqdm(start_cells, desc="ç”Ÿæˆè½¨è¿¹"):
        trajectory, metadata = generate_trajectory(
            model, x_start, time_points, obs_data, device=device
        )
        trajectories_list.append(trajectory)
        metadata_list.append(metadata)
        
        # ä¿å­˜å•æ¡è½¨è¿¹
        traj_output = os.path.join(args.output_dir, f"trajectory_{cell_id}.npy")
        np.save(traj_output, trajectory)
    
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼å…± {len(trajectories_list)} æ¡è½¨è¿¹")
    
    # 4. å¯è§†åŒ–
    print("ğŸ¨ å¯è§†åŒ–è½¨è¿¹...")
    vis_output = os.path.join(args.output_dir, "trajectories_visualization.png")
    visualize_trajectories(trajectories_list, metadata_list, vis_output)
    
    # 5. ä¿å­˜æ±‡æ€»
    summary_output = os.path.join(args.output_dir, "all_trajectories.npz")
    np.savez(
        summary_output,
        trajectories=np.array(trajectories_list),
        time_points=np.array(time_points)
    )
    print(f"âœ… è½¨è¿¹æ±‡æ€»ä¿å­˜åˆ°: {summary_output}")
    
    print("\nğŸ‰ å®Œæˆï¼")


if __name__ == "__main__":
    main()

